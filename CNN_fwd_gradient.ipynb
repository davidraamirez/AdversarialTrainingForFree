{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidraamirez/GradientWithoutBackpropagation/blob/main/CNN_fwd_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Import the necessary packages**"
      ],
      "metadata": {
        "id": "qkzEBdcD3BRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tqdm\n",
        "import torch.distributions as distr"
      ],
      "metadata": {
        "id": "dM-jay923zDH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchmetrics --quiet"
      ],
      "metadata": {
        "id": "2DoBHL2Tz2s3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "h5aLBvC5z3kK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "g3fVQdpkKkxM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functorch as fc\n",
        "from functorch import jvp\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "M29moBPxQL_g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "0NmFS6q0Kq5Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading and preprocessing the data**"
      ],
      "metadata": {
        "id": "w_w7gA2P5bQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "train_data = torchvision.datasets.KMNIST('./data', train=True, download=True)"
      ],
      "metadata": {
        "id": "YNmPBpfS3FYq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This loads data with data conversion.\n",
        "train_data = torchvision.datasets.KMNIST('./data', train=True, transform=T.ToTensor())"
      ],
      "metadata": {
        "id": "SLtrWriJ07-S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We visualize the first sample of the dataset\n",
        "plt.imshow(train_data.data[0])\n",
        "print(train_data.targets[0])"
      ],
      "metadata": {
        "id": "jVI_A1TvK_RO",
        "outputId": "c481bcb3-e462-4b78-e32b-91db5bea6312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the loader to shuffle the elements of the dataset\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True)"
      ],
      "metadata": {
        "id": "L9a4PEGN0SUD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get the first element of the loader\n",
        "xb, yb = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "0HkrrE4f0f7K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the testset\n",
        "test_data = torchvision.datasets.KMNIST('./data', train=False, transform=T.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=False)"
      ],
      "metadata": {
        "id": "-acBNiYh1KxY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Convolutional Neural Network**"
      ],
      "metadata": {
        "id": "T55GS3b95kKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_size, 2, 3, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(2, 4, 3, padding=1)\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(4*14*14, 64)\n",
        "\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.max_pool(x)\n",
        "        x = x.reshape((-1, 4*14*14))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x.reshape(1, -1)\n",
        "        return torch.softmax(x, 1)"
      ],
      "metadata": {
        "id": "ssgUZF_S8gmM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We check if CUDA is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ9BguV01cPi",
        "outputId": "47209ee9-81b5-4b33-e5e2-f544d66cad43"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize the parameters**"
      ],
      "metadata": {
        "id": "jgPnXQqq54pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We initialize the parameters randomly and the model with an input size\n",
        "#conv1w = torch.FloatTensor(8, 1, 3, 3).uniform_(-1, 1)\n",
        "#conv1b = torch.randint(-1, 1, (8, ), dtype=torch.float32, requires_grad=False)\n",
        "#conv2w = torch.randint(-1, 1, (16, 8, 3, 3), dtype=torch.float32, requires_grad=False)\n",
        "#conv2b = torch.randint(-1, 1, (16, ), dtype=torch.float32, requires_grad=False)\n",
        "#fc1w = torch.randint(-1, 1, (1024, 3136), dtype=torch.float32, requires_grad=False)\n",
        "#fc1b = torch.randint(-1, 1, (1024, ), dtype=torch.float32, requires_grad=False)\n",
        "#fc2w = torch.randint(-1, 1, (10, 1024), dtype=torch.float32, requires_grad=False)\n",
        "#fc2b = torch.randint(-1, 1, (10, ), dtype=torch.float32, requires_grad=False)\n",
        "cnn = SimpleCNN(1).to(device)"
      ],
      "metadata": {
        "id": "Exuz9TVC3ReH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Get the functional version of the model with functorch\n",
        "fcnn, params = fc.make_functional(cnn)\n",
        "fcnn(params, xb)"
      ],
      "metadata": {
        "id": "sfvgncI2QCnx",
        "outputId": "e2e965e1-c344-42b0-e865-f95ec44ef2f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1104, 0.1004, 0.0909, 0.0946, 0.0962, 0.0908, 0.1036, 0.1031, 0.1147,\n",
              "         0.0952]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We try our model with the first example\n",
        "print(cnn(xb.to(device))[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBOLO2Ee3ZwE",
        "outputId": "5b0b1328-f816-4264-d538-5fc323e1ed74"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1104, 0.1004, 0.0909, 0.0946, 0.0962, 0.0908, 0.1036, 0.1031, 0.1147,\n",
            "        0.0952], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate predictions"
      ],
      "metadata": {
        "id": "Wm-hZ3wXTHfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(sample_size, loader, model):\n",
        "  Xtrain = torch.randn(sample_size, 1, 28, 28)\n",
        "  ytrain = torch.randn(sample_size)\n",
        "  ypred = torch.randn(sample_size, 10)\n",
        "\n",
        "  for i in range(sample_size):\n",
        "    xb, yb = next(iter(loader))\n",
        "    Xtrain[i] = xb\n",
        "    ytrain[i] = yb\n",
        "    ypred[i] = model(xb)\n",
        "  return Xtrain, ytrain, ypred"
      ],
      "metadata": {
        "id": "00df8WdlTNHG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, ytrain, ypred = pred(500, train_loader, cnn)"
      ],
      "metadata": {
        "id": "s1JIt95nWEeM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define accuracy**"
      ],
      "metadata": {
        "id": "TIaVzpBW5th5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(ytrue, ypred):\n",
        "  return (ypred.argmax(1) == ytrue.long()).float().mean()"
      ],
      "metadata": {
        "id": "oY45ByC93hg7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average accuracy at initialization is 10% (random guessing).\n",
        "accuracy(ytrain, ypred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNh5yF2Q53Tv",
        "outputId": "26e9acf5-1833-4ac7-ed13-a797b3b31ec8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Cross-Entropy**"
      ],
      "metadata": {
        "id": "vYbOZm8oUoTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(params, fmodel, ytrue, x):\n",
        "  ypred=torch.randn((ytrue.shape[0],10))\n",
        "  for j in range(ytrue.shape[0]):\n",
        "    ypred[j] = fmodel(params, x[j])\n",
        "  return - ypred[torch.arange(0, ypred.shape[0]), ytrue.long()].log().mean()"
      ],
      "metadata": {
        "id": "xDw2fEWZUslj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_entropy(params, fcnn, ytrain, Xtrain))"
      ],
      "metadata": {
        "id": "9JLiJvzSV-_0",
        "outputId": "10407947-beef-47b8-fc6d-3132cf56eedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3142, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Other optimization functions**"
      ],
      "metadata": {
        "id": "2qZ3IMitSOpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beale_function(x):\n",
        "  return (torch.pow(torch.tensor([1.5])-x[0]+x[0]*x[1],2) + torch.pow(torch.tensor([2.25])-x[0]+x[0]*torch.pow(x[1],2),2)+torch.pow(torch.tensor([2.625])-x[0]+x[0]*torch.pow(x[1],3),2))"
      ],
      "metadata": {
        "id": "1WtQ6zyjX6wD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rosenbrock_function(x):\n",
        "  sum=0\n",
        "  for p in x.size():\n",
        "    for i in range (x.size(1)-1):\n",
        "      sum += (100*torch.pow(x[i+1] - torch.pow(x[i], 2), 2) + torch.pow(x[i]-1, 2))\n",
        "  return sum"
      ],
      "metadata": {
        "id": "KsZbhrFnAoK5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train and evaluate the network**"
      ],
      "metadata": {
        "id": "U12SFrBjSSmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fwd_gradient(train_loader):\n",
        "\n",
        "  l_rate0 = 0.05\n",
        "\n",
        "  cnn = SimpleCNN(1).to(device)\n",
        "  fcnn, params = fc.make_functional(cnn)\n",
        "  Xtrain, ytrain, ypred = pred(500, train_loader, cnn)\n",
        "\n",
        "  Xtest, ytest, ypredtest = pred(100, test_loader, cnn)\n",
        "\n",
        "  loss = cross_entropy(params, fcnn, ytrain, Xtrain)\n",
        "  t=0\n",
        "  t0 = time.time()\n",
        "\n",
        "  while (loss>0.5) :\n",
        "\n",
        "    v = tuple([torch.randn_like(p) for p in params])\n",
        "    g = partial(cross_entropy, fmodel = fcnn, ytrue=ytrain, x=Xtrain)\n",
        "\n",
        "    loss, dt = jvp(g, (params, ), (v, ))\n",
        "    print('dt', dt)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for j, p in enumerate(params):\n",
        "        gt = v[j]*dt\n",
        "        p -= l_rate0*gt\n",
        "\n",
        "    # We add the execution time of the iteration\n",
        "    t1=time.time()\n",
        "    t+=t1-t0\n",
        "    t0=t1\n",
        "    print('Time', t, 'loss', loss)\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "1xtc8O-2D8YJ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = train_fwd_gradient(train_loader)"
      ],
      "metadata": {
        "id": "oc0xWYUkZTf9",
        "outputId": "252d5e3c-f169-4323-dc76-c5bff250e740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt tensor(0.0728, grad_fn=<NegBackward0>)\n",
            "Time 0.7180399894714355 loss tensor(2.3070, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0058, grad_fn=<NegBackward0>)\n",
            "Time 1.5466196537017822 loss tensor(2.3068, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0054, grad_fn=<NegBackward0>)\n",
            "Time 2.3913164138793945 loss tensor(2.3068, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0042, grad_fn=<NegBackward0>)\n",
            "Time 3.170753240585327 loss tensor(2.3068, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0332, grad_fn=<NegBackward0>)\n",
            "Time 4.296854734420776 loss tensor(2.3068, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0621, grad_fn=<NegBackward0>)\n",
            "Time 5.53041672706604 loss tensor(2.3067, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0525, grad_fn=<NegBackward0>)\n",
            "Time 6.559858798980713 loss tensor(2.3066, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0708, grad_fn=<NegBackward0>)\n",
            "Time 7.348798513412476 loss tensor(2.3064, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0824, grad_fn=<NegBackward0>)\n",
            "Time 8.203672409057617 loss tensor(2.3062, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0692, grad_fn=<NegBackward0>)\n",
            "Time 8.977100372314453 loss tensor(2.3059, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0679, grad_fn=<NegBackward0>)\n",
            "Time 9.766643285751343 loss tensor(2.3057, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1682, grad_fn=<NegBackward0>)\n",
            "Time 10.57328462600708 loss tensor(2.3056, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0203, grad_fn=<NegBackward0>)\n",
            "Time 11.377743482589722 loss tensor(2.3043, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0050, grad_fn=<NegBackward0>)\n",
            "Time 12.170246601104736 loss tensor(2.3043, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0898, grad_fn=<NegBackward0>)\n",
            "Time 12.988866329193115 loss tensor(2.3043, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0589, grad_fn=<NegBackward0>)\n",
            "Time 13.897241353988647 loss tensor(2.3042, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1107, grad_fn=<NegBackward0>)\n",
            "Time 14.814928770065308 loss tensor(2.3040, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0081, grad_fn=<NegBackward0>)\n",
            "Time 15.639360427856445 loss tensor(2.3036, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1013, grad_fn=<NegBackward0>)\n",
            "Time 16.835500478744507 loss tensor(2.3036, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0128, grad_fn=<NegBackward0>)\n",
            "Time 18.237468242645264 loss tensor(2.3032, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0888, grad_fn=<NegBackward0>)\n",
            "Time 19.332032442092896 loss tensor(2.3032, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0253, grad_fn=<NegBackward0>)\n",
            "Time 20.21242904663086 loss tensor(2.3029, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0109, grad_fn=<NegBackward0>)\n",
            "Time 21.039328575134277 loss tensor(2.3028, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0711, grad_fn=<NegBackward0>)\n",
            "Time 21.88214135169983 loss tensor(2.3028, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0650, grad_fn=<NegBackward0>)\n",
            "Time 22.726555824279785 loss tensor(2.3028, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0092, grad_fn=<NegBackward0>)\n",
            "Time 23.570510387420654 loss tensor(2.3025, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1178, grad_fn=<NegBackward0>)\n",
            "Time 24.468093395233154 loss tensor(2.3025, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0133, grad_fn=<NegBackward0>)\n",
            "Time 25.318398237228394 loss tensor(2.3019, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0542, grad_fn=<NegBackward0>)\n",
            "Time 26.1811740398407 loss tensor(2.3019, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0968, grad_fn=<NegBackward0>)\n",
            "Time 27.041898250579834 loss tensor(2.3018, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0515, grad_fn=<NegBackward0>)\n",
            "Time 27.882105350494385 loss tensor(2.3015, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0507, grad_fn=<NegBackward0>)\n",
            "Time 28.778724431991577 loss tensor(2.3014, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0346, grad_fn=<NegBackward0>)\n",
            "Time 30.1648371219635 loss tensor(2.3013, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0897, grad_fn=<NegBackward0>)\n",
            "Time 31.408767700195312 loss tensor(2.3012, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0941, grad_fn=<NegBackward0>)\n",
            "Time 32.22683906555176 loss tensor(2.3010, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0695, grad_fn=<NegBackward0>)\n",
            "Time 33.110060691833496 loss tensor(2.3009, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0467, grad_fn=<NegBackward0>)\n",
            "Time 34.032248735427856 loss tensor(2.3008, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0594, grad_fn=<NegBackward0>)\n",
            "Time 34.88437533378601 loss tensor(2.3007, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1133, grad_fn=<NegBackward0>)\n",
            "Time 35.72056794166565 loss tensor(2.3006, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0830, grad_fn=<NegBackward0>)\n",
            "Time 36.582597494125366 loss tensor(2.3001, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1344, grad_fn=<NegBackward0>)\n",
            "Time 37.43371272087097 loss tensor(2.3000, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0147, grad_fn=<NegBackward0>)\n",
            "Time 38.26537561416626 loss tensor(2.2994, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0452, grad_fn=<NegBackward0>)\n",
            "Time 39.12783455848694 loss tensor(2.2994, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0893, grad_fn=<NegBackward0>)\n",
            "Time 39.97456169128418 loss tensor(2.2993, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0024, grad_fn=<NegBackward0>)\n",
            "Time 40.942296504974365 loss tensor(2.2991, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0369, grad_fn=<NegBackward0>)\n",
            "Time 42.23944568634033 loss tensor(2.2991, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0275, grad_fn=<NegBackward0>)\n",
            "Time 43.67597961425781 loss tensor(2.2990, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0278, grad_fn=<NegBackward0>)\n",
            "Time 44.60197997093201 loss tensor(2.2990, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0410, grad_fn=<NegBackward0>)\n",
            "Time 45.49915385246277 loss tensor(2.2989, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0364, grad_fn=<NegBackward0>)\n",
            "Time 46.37561798095703 loss tensor(2.2989, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0709, grad_fn=<NegBackward0>)\n",
            "Time 47.20948028564453 loss tensor(2.2988, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1273, grad_fn=<NegBackward0>)\n",
            "Time 48.07372331619263 loss tensor(2.2987, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0556, grad_fn=<NegBackward0>)\n",
            "Time 49.86129593849182 loss tensor(2.2979, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0078, grad_fn=<NegBackward0>)\n",
            "Time 51.472655057907104 loss tensor(2.2978, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0889, grad_fn=<NegBackward0>)\n",
            "Time 52.432838916778564 loss tensor(2.2978, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0139, grad_fn=<NegBackward0>)\n",
            "Time 53.30220007896423 loss tensor(2.2975, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0984, grad_fn=<NegBackward0>)\n",
            "Time 54.29120135307312 loss tensor(2.2975, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0012, grad_fn=<NegBackward0>)\n",
            "Time 55.54511857032776 loss tensor(2.2970, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1028, grad_fn=<NegBackward0>)\n",
            "Time 56.723644971847534 loss tensor(2.2970, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0857, grad_fn=<NegBackward0>)\n",
            "Time 57.56823205947876 loss tensor(2.2966, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0286, grad_fn=<NegBackward0>)\n",
            "Time 58.44003176689148 loss tensor(2.2964, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0473, grad_fn=<NegBackward0>)\n",
            "Time 59.31916332244873 loss tensor(2.2963, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0007, grad_fn=<NegBackward0>)\n",
            "Time 60.17243218421936 loss tensor(2.2962, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1288, grad_fn=<NegBackward0>)\n",
            "Time 61.01094174385071 loss tensor(2.2962, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0175, grad_fn=<NegBackward0>)\n",
            "Time 61.858482360839844 loss tensor(2.2954, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1169, grad_fn=<NegBackward0>)\n",
            "Time 62.71130418777466 loss tensor(2.2954, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0467, grad_fn=<NegBackward0>)\n",
            "Time 63.552125692367554 loss tensor(2.2950, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1000, grad_fn=<NegBackward0>)\n",
            "Time 64.40512609481812 loss tensor(2.2949, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0627, grad_fn=<NegBackward0>)\n",
            "Time 65.26436638832092 loss tensor(2.2944, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0249, grad_fn=<NegBackward0>)\n",
            "Time 66.09167575836182 loss tensor(2.2942, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0441, grad_fn=<NegBackward0>)\n",
            "Time 67.11759328842163 loss tensor(2.2942, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0212, grad_fn=<NegBackward0>)\n",
            "Time 68.49960899353027 loss tensor(2.2941, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0143, grad_fn=<NegBackward0>)\n",
            "Time 69.57751202583313 loss tensor(2.2941, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0716, grad_fn=<NegBackward0>)\n",
            "Time 70.427419424057 loss tensor(2.2940, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0180, grad_fn=<NegBackward0>)\n",
            "Time 71.27456498146057 loss tensor(2.2939, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0532, grad_fn=<NegBackward0>)\n",
            "Time 72.15616178512573 loss tensor(2.2939, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0591, grad_fn=<NegBackward0>)\n",
            "Time 73.10338425636292 loss tensor(2.2938, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0557, grad_fn=<NegBackward0>)\n",
            "Time 73.94396829605103 loss tensor(2.2938, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0155, grad_fn=<NegBackward0>)\n",
            "Time 74.79443764686584 loss tensor(2.2937, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0326, grad_fn=<NegBackward0>)\n",
            "Time 75.728768825531 loss tensor(2.2937, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0555, grad_fn=<NegBackward0>)\n",
            "Time 76.70658373832703 loss tensor(2.2936, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0595, grad_fn=<NegBackward0>)\n",
            "Time 77.58755373954773 loss tensor(2.2935, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0361, grad_fn=<NegBackward0>)\n",
            "Time 78.44477415084839 loss tensor(2.2934, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0029, grad_fn=<NegBackward0>)\n",
            "Time 79.38246655464172 loss tensor(2.2933, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1163, grad_fn=<NegBackward0>)\n",
            "Time 80.70081853866577 loss tensor(2.2933, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0332, grad_fn=<NegBackward0>)\n",
            "Time 82.01343846321106 loss tensor(2.2930, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0083, grad_fn=<NegBackward0>)\n",
            "Time 82.8637375831604 loss tensor(2.2929, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0559, grad_fn=<NegBackward0>)\n",
            "Time 83.73234629631042 loss tensor(2.2929, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0176, grad_fn=<NegBackward0>)\n",
            "Time 84.56618094444275 loss tensor(2.2928, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0302, grad_fn=<NegBackward0>)\n",
            "Time 85.4195008277893 loss tensor(2.2928, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0293, grad_fn=<NegBackward0>)\n",
            "Time 86.29249143600464 loss tensor(2.2927, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0406, grad_fn=<NegBackward0>)\n",
            "Time 87.1457347869873 loss tensor(2.2927, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0433, grad_fn=<NegBackward0>)\n",
            "Time 87.9874312877655 loss tensor(2.2926, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0172, grad_fn=<NegBackward0>)\n",
            "Time 88.8539469242096 loss tensor(2.2925, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0096, grad_fn=<NegBackward0>)\n",
            "Time 89.70402956008911 loss tensor(2.2925, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0545, grad_fn=<NegBackward0>)\n",
            "Time 90.5609540939331 loss tensor(2.2925, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0145, grad_fn=<NegBackward0>)\n",
            "Time 91.41631889343262 loss tensor(2.2924, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0309, grad_fn=<NegBackward0>)\n",
            "Time 92.5510528087616 loss tensor(2.2924, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0187, grad_fn=<NegBackward0>)\n",
            "Time 93.85265445709229 loss tensor(2.2924, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0106, grad_fn=<NegBackward0>)\n",
            "Time 94.89690804481506 loss tensor(2.2924, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0737, grad_fn=<NegBackward0>)\n",
            "Time 95.72171401977539 loss tensor(2.2924, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0495, grad_fn=<NegBackward0>)\n",
            "Time 96.57744812965393 loss tensor(2.2922, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0658, grad_fn=<NegBackward0>)\n",
            "Time 97.43080830574036 loss tensor(2.2921, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0289, grad_fn=<NegBackward0>)\n",
            "Time 98.26157665252686 loss tensor(2.2919, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0075, grad_fn=<NegBackward0>)\n",
            "Time 99.11620044708252 loss tensor(2.2918, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0572, grad_fn=<NegBackward0>)\n",
            "Time 100.02700805664062 loss tensor(2.2918, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0061, grad_fn=<NegBackward0>)\n",
            "Time 100.86904430389404 loss tensor(2.2918, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0388, grad_fn=<NegBackward0>)\n",
            "Time 101.7083945274353 loss tensor(2.2918, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0033, grad_fn=<NegBackward0>)\n",
            "Time 102.55085015296936 loss tensor(2.2918, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0177, grad_fn=<NegBackward0>)\n",
            "Time 103.41807842254639 loss tensor(2.2918, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0365, grad_fn=<NegBackward0>)\n",
            "Time 104.26695251464844 loss tensor(2.2918, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0077, grad_fn=<NegBackward0>)\n",
            "Time 105.51894187927246 loss tensor(2.2917, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0634, grad_fn=<NegBackward0>)\n",
            "Time 106.87776064872742 loss tensor(2.2917, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0013, grad_fn=<NegBackward0>)\n",
            "Time 107.75242161750793 loss tensor(2.2916, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0326, grad_fn=<NegBackward0>)\n",
            "Time 108.63746118545532 loss tensor(2.2916, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0363, grad_fn=<NegBackward0>)\n",
            "Time 109.61517238616943 loss tensor(2.2915, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0153, grad_fn=<NegBackward0>)\n",
            "Time 110.49988889694214 loss tensor(2.2915, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0028, grad_fn=<NegBackward0>)\n",
            "Time 111.35132956504822 loss tensor(2.2914, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0082, grad_fn=<NegBackward0>)\n",
            "Time 112.19074726104736 loss tensor(2.2914, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0123, grad_fn=<NegBackward0>)\n",
            "Time 113.03143644332886 loss tensor(2.2914, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0449, grad_fn=<NegBackward0>)\n",
            "Time 113.87028098106384 loss tensor(2.2914, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0166, grad_fn=<NegBackward0>)\n",
            "Time 114.74863386154175 loss tensor(2.2914, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0967, grad_fn=<NegBackward0>)\n",
            "Time 115.73489689826965 loss tensor(2.2913, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0236, grad_fn=<NegBackward0>)\n",
            "Time 116.62295246124268 loss tensor(2.2911, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0410, grad_fn=<NegBackward0>)\n",
            "Time 117.85611319541931 loss tensor(2.2910, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0357, grad_fn=<NegBackward0>)\n",
            "Time 119.2606909275055 loss tensor(2.2910, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0233, grad_fn=<NegBackward0>)\n",
            "Time 120.45128512382507 loss tensor(2.2910, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0920, grad_fn=<NegBackward0>)\n",
            "Time 121.42119765281677 loss tensor(2.2909, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0223, grad_fn=<NegBackward0>)\n",
            "Time 122.27474689483643 loss tensor(2.2906, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0169, grad_fn=<NegBackward0>)\n",
            "Time 123.16807556152344 loss tensor(2.2906, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0307, grad_fn=<NegBackward0>)\n",
            "Time 123.99932384490967 loss tensor(2.2906, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0090, grad_fn=<NegBackward0>)\n",
            "Time 124.85609793663025 loss tensor(2.2905, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0010, grad_fn=<NegBackward0>)\n",
            "Time 125.71283864974976 loss tensor(2.2905, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0148, grad_fn=<NegBackward0>)\n",
            "Time 126.69541597366333 loss tensor(2.2905, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0079, grad_fn=<NegBackward0>)\n",
            "Time 127.6394693851471 loss tensor(2.2905, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0158, grad_fn=<NegBackward0>)\n",
            "Time 128.54508781433105 loss tensor(2.2905, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0308, grad_fn=<NegBackward0>)\n",
            "Time 129.50358057022095 loss tensor(2.2905, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0059, grad_fn=<NegBackward0>)\n",
            "Time 130.90405702590942 loss tensor(2.2905, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0316, grad_fn=<NegBackward0>)\n",
            "Time 132.2954545021057 loss tensor(2.2905, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0542, grad_fn=<NegBackward0>)\n",
            "Time 133.3483109474182 loss tensor(2.2904, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0196, grad_fn=<NegBackward0>)\n",
            "Time 134.1916630268097 loss tensor(2.2903, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0357, grad_fn=<NegBackward0>)\n",
            "Time 135.22520422935486 loss tensor(2.2903, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0375, grad_fn=<NegBackward0>)\n",
            "Time 136.08994841575623 loss tensor(2.2903, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0369, grad_fn=<NegBackward0>)\n",
            "Time 136.93172550201416 loss tensor(2.2902, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0113, grad_fn=<NegBackward0>)\n",
            "Time 137.79693937301636 loss tensor(2.2902, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0290, grad_fn=<NegBackward0>)\n",
            "Time 138.64996695518494 loss tensor(2.2902, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0198, grad_fn=<NegBackward0>)\n",
            "Time 139.51624059677124 loss tensor(2.2901, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0079, grad_fn=<NegBackward0>)\n",
            "Time 140.37290716171265 loss tensor(2.2901, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0244, grad_fn=<NegBackward0>)\n",
            "Time 141.23367404937744 loss tensor(2.2901, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0170, grad_fn=<NegBackward0>)\n",
            "Time 142.1619336605072 loss tensor(2.2901, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0255, grad_fn=<NegBackward0>)\n",
            "Time 143.38042092323303 loss tensor(2.2901, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0063, grad_fn=<NegBackward0>)\n",
            "Time 144.6339819431305 loss tensor(2.2901, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0356, grad_fn=<NegBackward0>)\n",
            "Time 145.61724591255188 loss tensor(2.2901, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0186, grad_fn=<NegBackward0>)\n",
            "Time 146.44891166687012 loss tensor(2.2901, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0174, grad_fn=<NegBackward0>)\n",
            "Time 147.30318140983582 loss tensor(2.2900, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0442, grad_fn=<NegBackward0>)\n",
            "Time 148.1419961452484 loss tensor(2.2900, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0023, grad_fn=<NegBackward0>)\n",
            "Time 148.97191905975342 loss tensor(2.2900, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0445, grad_fn=<NegBackward0>)\n",
            "Time 149.82101154327393 loss tensor(2.2900, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0151, grad_fn=<NegBackward0>)\n",
            "Time 150.64833188056946 loss tensor(2.2899, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0006, grad_fn=<NegBackward0>)\n",
            "Time 151.47690224647522 loss tensor(2.2899, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0064, grad_fn=<NegBackward0>)\n",
            "Time 152.31114673614502 loss tensor(2.2899, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0407, grad_fn=<NegBackward0>)\n",
            "Time 153.20933532714844 loss tensor(2.2899, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0216, grad_fn=<NegBackward0>)\n",
            "Time 154.10146141052246 loss tensor(2.2898, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0007, grad_fn=<NegBackward0>)\n",
            "Time 154.98019218444824 loss tensor(2.2898, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0063, grad_fn=<NegBackward0>)\n",
            "Time 156.38990688323975 loss tensor(2.2898, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0260, grad_fn=<NegBackward0>)\n",
            "Time 157.66138005256653 loss tensor(2.2898, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0258, grad_fn=<NegBackward0>)\n",
            "Time 158.53440070152283 loss tensor(2.2898, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0296, grad_fn=<NegBackward0>)\n",
            "Time 159.4619574546814 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0081, grad_fn=<NegBackward0>)\n",
            "Time 160.352383852005 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0053, grad_fn=<NegBackward0>)\n",
            "Time 161.19437551498413 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0210, grad_fn=<NegBackward0>)\n",
            "Time 162.02786207199097 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0127, grad_fn=<NegBackward0>)\n",
            "Time 162.8535454273224 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0162, grad_fn=<NegBackward0>)\n",
            "Time 163.69728207588196 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0086, grad_fn=<NegBackward0>)\n",
            "Time 164.53124952316284 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-1.1412e-05, grad_fn=<NegBackward0>)\n",
            "Time 165.373868227005 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0205, grad_fn=<NegBackward0>)\n",
            "Time 166.24272394180298 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0005, grad_fn=<NegBackward0>)\n",
            "Time 167.0655267238617 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0271, grad_fn=<NegBackward0>)\n",
            "Time 168.0114779472351 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0037, grad_fn=<NegBackward0>)\n",
            "Time 169.44909739494324 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0265, grad_fn=<NegBackward0>)\n",
            "Time 170.65960431098938 loss tensor(2.2897, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0364, grad_fn=<NegBackward0>)\n",
            "Time 171.48833060264587 loss tensor(2.2896, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0275, grad_fn=<NegBackward0>)\n",
            "Time 172.35839986801147 loss tensor(2.2896, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0216, grad_fn=<NegBackward0>)\n",
            "Time 173.19757294654846 loss tensor(2.2895, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0075, grad_fn=<NegBackward0>)\n",
            "Time 174.02539324760437 loss tensor(2.2895, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0310, grad_fn=<NegBackward0>)\n",
            "Time 174.86523866653442 loss tensor(2.2895, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-1.4679e-05, grad_fn=<NegBackward0>)\n",
            "Time 175.72044014930725 loss tensor(2.2895, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0022, grad_fn=<NegBackward0>)\n",
            "Time 176.54988884925842 loss tensor(2.2895, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0528, grad_fn=<NegBackward0>)\n",
            "Time 177.3920018672943 loss tensor(2.2895, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0567, grad_fn=<NegBackward0>)\n",
            "Time 178.24391913414001 loss tensor(2.2894, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0176, grad_fn=<NegBackward0>)\n",
            "Time 179.07317566871643 loss tensor(2.2892, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0235, grad_fn=<NegBackward0>)\n",
            "Time 179.92663097381592 loss tensor(2.2892, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0265, grad_fn=<NegBackward0>)\n",
            "Time 180.9870569705963 loss tensor(2.2892, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0163, grad_fn=<NegBackward0>)\n",
            "Time 182.36387133598328 loss tensor(2.2892, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0380, grad_fn=<NegBackward0>)\n",
            "Time 183.46635603904724 loss tensor(2.2892, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0150, grad_fn=<NegBackward0>)\n",
            "Time 184.32574200630188 loss tensor(2.2891, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0245, grad_fn=<NegBackward0>)\n",
            "Time 185.238098859787 loss tensor(2.2892, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0387, grad_fn=<NegBackward0>)\n",
            "Time 186.2177119255066 loss tensor(2.2891, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0306, grad_fn=<NegBackward0>)\n",
            "Time 187.1937153339386 loss tensor(2.2891, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0388, grad_fn=<NegBackward0>)\n",
            "Time 188.0350751876831 loss tensor(2.2890, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0839, grad_fn=<NegBackward0>)\n",
            "Time 188.99961280822754 loss tensor(2.2890, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0012, grad_fn=<NegBackward0>)\n",
            "Time 189.97886037826538 loss tensor(2.2889, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0143, grad_fn=<NegBackward0>)\n",
            "Time 190.91591572761536 loss tensor(2.2889, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0448, grad_fn=<NegBackward0>)\n",
            "Time 191.75583624839783 loss tensor(2.2889, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0221, grad_fn=<NegBackward0>)\n",
            "Time 192.57275366783142 loss tensor(2.2889, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0022, grad_fn=<NegBackward0>)\n",
            "Time 193.7814815044403 loss tensor(2.2888, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0148, grad_fn=<NegBackward0>)\n",
            "Time 195.02545070648193 loss tensor(2.2888, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0064, grad_fn=<NegBackward0>)\n",
            "Time 196.06410813331604 loss tensor(2.2888, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0112, grad_fn=<NegBackward0>)\n",
            "Time 196.91009140014648 loss tensor(2.2888, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0183, grad_fn=<NegBackward0>)\n",
            "Time 197.73082756996155 loss tensor(2.2888, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0298, grad_fn=<NegBackward0>)\n",
            "Time 198.55912518501282 loss tensor(2.2888, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0433, grad_fn=<NegBackward0>)\n",
            "Time 199.42900848388672 loss tensor(2.2888, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0463, grad_fn=<NegBackward0>)\n",
            "Time 200.27508997917175 loss tensor(2.2888, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0155, grad_fn=<NegBackward0>)\n",
            "Time 201.12277817726135 loss tensor(2.2887, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0076, grad_fn=<NegBackward0>)\n",
            "Time 201.94778966903687 loss tensor(2.2887, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0196, grad_fn=<NegBackward0>)\n",
            "Time 202.7736518383026 loss tensor(2.2887, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0353, grad_fn=<NegBackward0>)\n",
            "Time 203.62589740753174 loss tensor(2.2887, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0209, grad_fn=<NegBackward0>)\n",
            "Time 204.5557098388672 loss tensor(2.2886, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0071, grad_fn=<NegBackward0>)\n",
            "Time 205.53251767158508 loss tensor(2.2886, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0574, grad_fn=<NegBackward0>)\n",
            "Time 206.81028366088867 loss tensor(2.2886, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0102, grad_fn=<NegBackward0>)\n",
            "Time 208.1916127204895 loss tensor(2.2886, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0149, grad_fn=<NegBackward0>)\n",
            "Time 209.18139123916626 loss tensor(2.2886, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0051, grad_fn=<NegBackward0>)\n",
            "Time 210.11513924598694 loss tensor(2.2886, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0481, grad_fn=<NegBackward0>)\n",
            "Time 210.94696974754333 loss tensor(2.2885, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0229, grad_fn=<NegBackward0>)\n",
            "Time 211.80106329917908 loss tensor(2.2885, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0379, grad_fn=<NegBackward0>)\n",
            "Time 212.70215845108032 loss tensor(2.2885, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0006, grad_fn=<NegBackward0>)\n",
            "Time 213.6772654056549 loss tensor(2.2884, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0229, grad_fn=<NegBackward0>)\n",
            "Time 214.6580765247345 loss tensor(2.2884, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0364, grad_fn=<NegBackward0>)\n",
            "Time 215.56185603141785 loss tensor(2.2884, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0075, grad_fn=<NegBackward0>)\n",
            "Time 216.40289855003357 loss tensor(2.2884, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0318, grad_fn=<NegBackward0>)\n",
            "Time 217.258629322052 loss tensor(2.2884, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0585, grad_fn=<NegBackward0>)\n",
            "Time 218.1918306350708 loss tensor(2.2884, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0085, grad_fn=<NegBackward0>)\n",
            "Time 219.3923692703247 loss tensor(2.2883, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0054, grad_fn=<NegBackward0>)\n",
            "Time 220.6741373538971 loss tensor(2.2883, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0052, grad_fn=<NegBackward0>)\n",
            "Time 221.67901301383972 loss tensor(2.2883, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0200, grad_fn=<NegBackward0>)\n",
            "Time 222.5245316028595 loss tensor(2.2883, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0104, grad_fn=<NegBackward0>)\n",
            "Time 223.82188367843628 loss tensor(2.2883, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0047, grad_fn=<NegBackward0>)\n",
            "Time 225.5820508003235 loss tensor(2.2883, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0184, grad_fn=<NegBackward0>)\n",
            "Time 226.46848320960999 loss tensor(2.2883, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0380, grad_fn=<NegBackward0>)\n",
            "Time 227.427170753479 loss tensor(2.2883, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0198, grad_fn=<NegBackward0>)\n",
            "Time 228.3034632205963 loss tensor(2.2882, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0199, grad_fn=<NegBackward0>)\n",
            "Time 229.2550187110901 loss tensor(2.2882, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0071, grad_fn=<NegBackward0>)\n",
            "Time 230.14167714118958 loss tensor(2.2882, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0324, grad_fn=<NegBackward0>)\n",
            "Time 230.99085807800293 loss tensor(2.2882, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0238, grad_fn=<NegBackward0>)\n",
            "Time 232.23205280303955 loss tensor(2.2881, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0481, grad_fn=<NegBackward0>)\n",
            "Time 233.46222019195557 loss tensor(2.2881, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0165, grad_fn=<NegBackward0>)\n",
            "Time 234.59651923179626 loss tensor(2.2880, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0215, grad_fn=<NegBackward0>)\n",
            "Time 235.4930853843689 loss tensor(2.2880, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0301, grad_fn=<NegBackward0>)\n",
            "Time 236.46543860435486 loss tensor(2.2880, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0050, grad_fn=<NegBackward0>)\n",
            "Time 237.46480774879456 loss tensor(2.2880, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0141, grad_fn=<NegBackward0>)\n",
            "Time 239.64392805099487 loss tensor(2.2880, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0218, grad_fn=<NegBackward0>)\n",
            "Time 240.58894968032837 loss tensor(2.2880, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0119, grad_fn=<NegBackward0>)\n",
            "Time 241.55584597587585 loss tensor(2.2879, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0057, grad_fn=<NegBackward0>)\n",
            "Time 242.44894289970398 loss tensor(2.2879, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0063, grad_fn=<NegBackward0>)\n",
            "Time 243.29039764404297 loss tensor(2.2879, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0440, grad_fn=<NegBackward0>)\n",
            "Time 245.52556347846985 loss tensor(2.2879, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0449, grad_fn=<NegBackward0>)\n",
            "Time 247.05324292182922 loss tensor(2.2879, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0202, grad_fn=<NegBackward0>)\n",
            "Time 247.9936399459839 loss tensor(2.2878, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0050, grad_fn=<NegBackward0>)\n",
            "Time 248.94783568382263 loss tensor(2.2878, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0129, grad_fn=<NegBackward0>)\n",
            "Time 249.916601896286 loss tensor(2.2878, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0126, grad_fn=<NegBackward0>)\n",
            "Time 250.77368474006653 loss tensor(2.2878, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0462, grad_fn=<NegBackward0>)\n",
            "Time 251.6000943183899 loss tensor(2.2878, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0101, grad_fn=<NegBackward0>)\n",
            "Time 252.48782968521118 loss tensor(2.2878, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0403, grad_fn=<NegBackward0>)\n",
            "Time 253.44111251831055 loss tensor(2.2878, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0124, grad_fn=<NegBackward0>)\n",
            "Time 254.31153798103333 loss tensor(2.2877, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0581, grad_fn=<NegBackward0>)\n",
            "Time 255.16260695457458 loss tensor(2.2877, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0221, grad_fn=<NegBackward0>)\n",
            "Time 255.9947452545166 loss tensor(2.2876, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0617, grad_fn=<NegBackward0>)\n",
            "Time 257.0374972820282 loss tensor(2.2876, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0161, grad_fn=<NegBackward0>)\n",
            "Time 258.4197473526001 loss tensor(2.2876, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0141, grad_fn=<NegBackward0>)\n",
            "Time 259.62725138664246 loss tensor(2.2876, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0500, grad_fn=<NegBackward0>)\n",
            "Time 260.48159646987915 loss tensor(2.2875, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0141, grad_fn=<NegBackward0>)\n",
            "Time 261.37181854248047 loss tensor(2.2875, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0532, grad_fn=<NegBackward0>)\n",
            "Time 262.361426115036 loss tensor(2.2875, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0501, grad_fn=<NegBackward0>)\n",
            "Time 263.215797662735 loss tensor(2.2873, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0203, grad_fn=<NegBackward0>)\n",
            "Time 264.0550684928894 loss tensor(2.2873, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0120, grad_fn=<NegBackward0>)\n",
            "Time 265.0112509727478 loss tensor(2.2873, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0181, grad_fn=<NegBackward0>)\n",
            "Time 266.46088886260986 loss tensor(2.2873, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0051, grad_fn=<NegBackward0>)\n",
            "Time 267.3664598464966 loss tensor(2.2873, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0038, grad_fn=<NegBackward0>)\n",
            "Time 268.22332191467285 loss tensor(2.2873, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0591, grad_fn=<NegBackward0>)\n",
            "Time 269.0623893737793 loss tensor(2.2873, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0026, grad_fn=<NegBackward0>)\n",
            "Time 270.14207577705383 loss tensor(2.2872, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0208, grad_fn=<NegBackward0>)\n",
            "Time 271.34821224212646 loss tensor(2.2872, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0053, grad_fn=<NegBackward0>)\n",
            "Time 273.047803401947 loss tensor(2.2872, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0188, grad_fn=<NegBackward0>)\n",
            "Time 273.996205329895 loss tensor(2.2872, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0143, grad_fn=<NegBackward0>)\n",
            "Time 274.9015130996704 loss tensor(2.2872, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0431, grad_fn=<NegBackward0>)\n",
            "Time 275.82443928718567 loss tensor(2.2872, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0921, grad_fn=<NegBackward0>)\n",
            "Time 276.7854266166687 loss tensor(2.2872, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0546, grad_fn=<NegBackward0>)\n",
            "Time 277.71158266067505 loss tensor(2.2869, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0055, grad_fn=<NegBackward0>)\n",
            "Time 278.5806555747986 loss tensor(2.2869, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0419, grad_fn=<NegBackward0>)\n",
            "Time 279.42860531806946 loss tensor(2.2869, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0359, grad_fn=<NegBackward0>)\n",
            "Time 280.25126457214355 loss tensor(2.2868, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0772, grad_fn=<NegBackward0>)\n",
            "Time 281.11373472213745 loss tensor(2.2868, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0297, grad_fn=<NegBackward0>)\n",
            "Time 281.9951434135437 loss tensor(2.2868, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0144, grad_fn=<NegBackward0>)\n",
            "Time 283.08390760421753 loss tensor(2.2867, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0193, grad_fn=<NegBackward0>)\n",
            "Time 284.4099175930023 loss tensor(2.2867, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0170, grad_fn=<NegBackward0>)\n",
            "Time 285.43232440948486 loss tensor(2.2867, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0296, grad_fn=<NegBackward0>)\n",
            "Time 286.3409333229065 loss tensor(2.2867, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0428, grad_fn=<NegBackward0>)\n",
            "Time 287.24446725845337 loss tensor(2.2867, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0614, grad_fn=<NegBackward0>)\n",
            "Time 288.2462477684021 loss tensor(2.2866, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0514, grad_fn=<NegBackward0>)\n",
            "Time 289.08236289024353 loss tensor(2.2865, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0708, grad_fn=<NegBackward0>)\n",
            "Time 289.90682721138 loss tensor(2.2864, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0299, grad_fn=<NegBackward0>)\n",
            "Time 290.8496572971344 loss tensor(2.2865, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0012, grad_fn=<NegBackward0>)\n",
            "Time 291.71201395988464 loss tensor(2.2865, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0315, grad_fn=<NegBackward0>)\n",
            "Time 292.5417699813843 loss tensor(2.2865, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0221, grad_fn=<NegBackward0>)\n",
            "Time 293.3836953639984 loss tensor(2.2864, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0291, grad_fn=<NegBackward0>)\n",
            "Time 294.2181453704834 loss tensor(2.2864, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0268, grad_fn=<NegBackward0>)\n",
            "Time 295.1451585292816 loss tensor(2.2864, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0123, grad_fn=<NegBackward0>)\n",
            "Time 296.4823558330536 loss tensor(2.2864, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0452, grad_fn=<NegBackward0>)\n",
            "Time 297.67773175239563 loss tensor(2.2864, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1642, grad_fn=<NegBackward0>)\n",
            "Time 298.52291798591614 loss tensor(2.2864, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0039, grad_fn=<NegBackward0>)\n",
            "Time 299.42092061042786 loss tensor(2.2860, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0466, grad_fn=<NegBackward0>)\n",
            "Time 300.3621711730957 loss tensor(2.2860, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0172, grad_fn=<NegBackward0>)\n",
            "Time 301.2051339149475 loss tensor(2.2860, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0428, grad_fn=<NegBackward0>)\n",
            "Time 302.05859661102295 loss tensor(2.2859, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0274, grad_fn=<NegBackward0>)\n",
            "Time 302.8936879634857 loss tensor(2.2859, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0346, grad_fn=<NegBackward0>)\n",
            "Time 303.6971604824066 loss tensor(2.2859, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0442, grad_fn=<NegBackward0>)\n",
            "Time 304.565260887146 loss tensor(2.2858, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0893, grad_fn=<NegBackward0>)\n",
            "Time 305.48905539512634 loss tensor(2.2857, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0403, grad_fn=<NegBackward0>)\n",
            "Time 306.44667768478394 loss tensor(2.2854, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0652, grad_fn=<NegBackward0>)\n",
            "Time 307.42604780197144 loss tensor(2.2853, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0471, grad_fn=<NegBackward0>)\n",
            "Time 308.5949800014496 loss tensor(2.2853, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0898, grad_fn=<NegBackward0>)\n",
            "Time 309.80339074134827 loss tensor(2.2853, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0233, grad_fn=<NegBackward0>)\n",
            "Time 310.75478076934814 loss tensor(2.2852, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1767, grad_fn=<NegBackward0>)\n",
            "Time 311.57546496391296 loss tensor(2.2852, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0127, grad_fn=<NegBackward0>)\n",
            "Time 312.41542983055115 loss tensor(2.2844, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0109, grad_fn=<NegBackward0>)\n",
            "Time 313.26121973991394 loss tensor(2.2844, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0165, grad_fn=<NegBackward0>)\n",
            "Time 314.1096365451813 loss tensor(2.2844, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0343, grad_fn=<NegBackward0>)\n",
            "Time 314.9428515434265 loss tensor(2.2844, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0070, grad_fn=<NegBackward0>)\n",
            "Time 315.7773988246918 loss tensor(2.2844, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0721, grad_fn=<NegBackward0>)\n",
            "Time 316.6164274215698 loss tensor(2.2844, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0103, grad_fn=<NegBackward0>)\n",
            "Time 317.55538964271545 loss tensor(2.2842, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0204, grad_fn=<NegBackward0>)\n",
            "Time 318.50472807884216 loss tensor(2.2842, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0162, grad_fn=<NegBackward0>)\n",
            "Time 319.3550429344177 loss tensor(2.2842, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0460, grad_fn=<NegBackward0>)\n",
            "Time 320.17848539352417 loss tensor(2.2842, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0357, grad_fn=<NegBackward0>)\n",
            "Time 321.4146046638489 loss tensor(2.2841, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0129, grad_fn=<NegBackward0>)\n",
            "Time 322.80490469932556 loss tensor(2.2841, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0235, grad_fn=<NegBackward0>)\n",
            "Time 323.6732847690582 loss tensor(2.2841, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0922, grad_fn=<NegBackward0>)\n",
            "Time 324.7039453983307 loss tensor(2.2840, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0267, grad_fn=<NegBackward0>)\n",
            "Time 325.71359395980835 loss tensor(2.2838, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0078, grad_fn=<NegBackward0>)\n",
            "Time 326.5849335193634 loss tensor(2.2838, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0405, grad_fn=<NegBackward0>)\n",
            "Time 327.5554721355438 loss tensor(2.2838, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0125, grad_fn=<NegBackward0>)\n",
            "Time 328.45024132728577 loss tensor(2.2838, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1510, grad_fn=<NegBackward0>)\n",
            "Time 329.4628369808197 loss tensor(2.2838, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1016, grad_fn=<NegBackward0>)\n",
            "Time 330.3750400543213 loss tensor(2.2830, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0510, grad_fn=<NegBackward0>)\n",
            "Time 331.23976612091064 loss tensor(2.2825, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1292, grad_fn=<NegBackward0>)\n",
            "Time 332.0800712108612 loss tensor(2.2824, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0353, grad_fn=<NegBackward0>)\n",
            "Time 332.9252371788025 loss tensor(2.2819, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1519, grad_fn=<NegBackward0>)\n",
            "Time 334.16471457481384 loss tensor(2.2818, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0523, grad_fn=<NegBackward0>)\n",
            "Time 335.4288737773895 loss tensor(2.2810, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0456, grad_fn=<NegBackward0>)\n",
            "Time 336.28478145599365 loss tensor(2.2809, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0746, grad_fn=<NegBackward0>)\n",
            "Time 337.1367344856262 loss tensor(2.2809, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0463, grad_fn=<NegBackward0>)\n",
            "Time 337.98096561431885 loss tensor(2.2808, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0801, grad_fn=<NegBackward0>)\n",
            "Time 338.8127272129059 loss tensor(2.2807, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1269, grad_fn=<NegBackward0>)\n",
            "Time 339.67787981033325 loss tensor(2.2804, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0243, grad_fn=<NegBackward0>)\n",
            "Time 340.5392060279846 loss tensor(2.2799, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0677, grad_fn=<NegBackward0>)\n",
            "Time 341.4530408382416 loss tensor(2.2799, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0440, grad_fn=<NegBackward0>)\n",
            "Time 342.3815803527832 loss tensor(2.2798, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0186, grad_fn=<NegBackward0>)\n",
            "Time 343.24160528182983 loss tensor(2.2797, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0718, grad_fn=<NegBackward0>)\n",
            "Time 344.255334854126 loss tensor(2.2797, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0453, grad_fn=<NegBackward0>)\n",
            "Time 345.0969581604004 loss tensor(2.2796, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0391, grad_fn=<NegBackward0>)\n",
            "Time 346.2030448913574 loss tensor(2.2795, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1303, grad_fn=<NegBackward0>)\n",
            "Time 347.44517946243286 loss tensor(2.2795, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0296, grad_fn=<NegBackward0>)\n",
            "Time 348.60624647140503 loss tensor(2.2794, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0226, grad_fn=<NegBackward0>)\n",
            "Time 349.63947057724 loss tensor(2.2794, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0627, grad_fn=<NegBackward0>)\n",
            "Time 350.60380959510803 loss tensor(2.2794, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0103, grad_fn=<NegBackward0>)\n",
            "Time 351.603711605072 loss tensor(2.2792, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0840, grad_fn=<NegBackward0>)\n",
            "Time 352.6411700248718 loss tensor(2.2792, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1220, grad_fn=<NegBackward0>)\n",
            "Time 353.6434545516968 loss tensor(2.2791, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0863, grad_fn=<NegBackward0>)\n",
            "Time 354.64414858818054 loss tensor(2.2785, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0029, grad_fn=<NegBackward0>)\n",
            "Time 355.6293783187866 loss tensor(2.2783, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0164, grad_fn=<NegBackward0>)\n",
            "Time 356.6249713897705 loss tensor(2.2783, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0448, grad_fn=<NegBackward0>)\n",
            "Time 357.57992601394653 loss tensor(2.2783, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1365, grad_fn=<NegBackward0>)\n",
            "Time 358.6741988658905 loss tensor(2.2783, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1463, grad_fn=<NegBackward0>)\n",
            "Time 360.08348870277405 loss tensor(2.2776, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0796, grad_fn=<NegBackward0>)\n",
            "Time 361.22407245635986 loss tensor(2.2771, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0375, grad_fn=<NegBackward0>)\n",
            "Time 362.1319954395294 loss tensor(2.2768, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0523, grad_fn=<NegBackward0>)\n",
            "Time 363.06276774406433 loss tensor(2.2769, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1430, grad_fn=<NegBackward0>)\n",
            "Time 364.06377387046814 loss tensor(2.2767, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0066, grad_fn=<NegBackward0>)\n",
            "Time 365.03700017929077 loss tensor(2.2763, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0806, grad_fn=<NegBackward0>)\n",
            "Time 366.0336184501648 loss tensor(2.2763, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1272, grad_fn=<NegBackward0>)\n",
            "Time 367.0286545753479 loss tensor(2.2761, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0136, grad_fn=<NegBackward0>)\n",
            "Time 368.03250527381897 loss tensor(2.2757, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0256, grad_fn=<NegBackward0>)\n",
            "Time 369.0303840637207 loss tensor(2.2757, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.2256, grad_fn=<NegBackward0>)\n",
            "Time 370.06478095054626 loss tensor(2.2756, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0621, grad_fn=<NegBackward0>)\n",
            "Time 371.23417258262634 loss tensor(2.2752, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0486, grad_fn=<NegBackward0>)\n",
            "Time 372.6006350517273 loss tensor(2.2750, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0008, grad_fn=<NegBackward0>)\n",
            "Time 373.8581576347351 loss tensor(2.2750, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0059, grad_fn=<NegBackward0>)\n",
            "Time 374.8917043209076 loss tensor(2.2750, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0428, grad_fn=<NegBackward0>)\n",
            "Time 375.81259655952454 loss tensor(2.2750, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1346, grad_fn=<NegBackward0>)\n",
            "Time 376.6783866882324 loss tensor(2.2750, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0623, grad_fn=<NegBackward0>)\n",
            "Time 377.6422302722931 loss tensor(2.2741, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0371, grad_fn=<NegBackward0>)\n",
            "Time 378.600088596344 loss tensor(2.2741, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0626, grad_fn=<NegBackward0>)\n",
            "Time 379.5189003944397 loss tensor(2.2740, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0989, grad_fn=<NegBackward0>)\n",
            "Time 380.3983166217804 loss tensor(2.2740, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0951, grad_fn=<NegBackward0>)\n",
            "Time 381.2505793571472 loss tensor(2.2737, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1194, grad_fn=<NegBackward0>)\n",
            "Time 382.09707403182983 loss tensor(2.2734, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0999, grad_fn=<NegBackward0>)\n",
            "Time 383.05241298675537 loss tensor(2.2730, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1616, grad_fn=<NegBackward0>)\n",
            "Time 384.3987023830414 loss tensor(2.2727, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0588, grad_fn=<NegBackward0>)\n",
            "Time 385.9379267692566 loss tensor(2.2720, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0257, grad_fn=<NegBackward0>)\n",
            "Time 386.8869960308075 loss tensor(2.2719, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1739, grad_fn=<NegBackward0>)\n",
            "Time 387.83805680274963 loss tensor(2.2719, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.2915, grad_fn=<NegBackward0>)\n",
            "Time 388.7561733722687 loss tensor(2.2713, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0501, grad_fn=<NegBackward0>)\n",
            "Time 389.65180492401123 loss tensor(2.2702, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1183, grad_fn=<NegBackward0>)\n",
            "Time 390.5071542263031 loss tensor(2.2703, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0597, grad_fn=<NegBackward0>)\n",
            "Time 391.34802627563477 loss tensor(2.2702, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1756, grad_fn=<NegBackward0>)\n",
            "Time 392.33796644210815 loss tensor(2.2700, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.2713, grad_fn=<NegBackward0>)\n",
            "Time 393.21844935417175 loss tensor(2.2693, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1352, grad_fn=<NegBackward0>)\n",
            "Time 394.0725128650665 loss tensor(2.2682, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.3453, grad_fn=<NegBackward0>)\n",
            "Time 395.0139639377594 loss tensor(2.2674, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0320, grad_fn=<NegBackward0>)\n",
            "Time 395.92910265922546 loss tensor(2.2657, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.3663, grad_fn=<NegBackward0>)\n",
            "Time 397.27238965034485 loss tensor(2.2656, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0213, grad_fn=<NegBackward0>)\n",
            "Time 398.65533423423767 loss tensor(2.2625, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1002, grad_fn=<NegBackward0>)\n",
            "Time 399.68393659591675 loss tensor(2.2625, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0512, grad_fn=<NegBackward0>)\n",
            "Time 400.68360567092896 loss tensor(2.2628, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1743, grad_fn=<NegBackward0>)\n",
            "Time 401.6176691055298 loss tensor(2.2628, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1888, grad_fn=<NegBackward0>)\n",
            "Time 402.6009986400604 loss tensor(2.2623, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1651, grad_fn=<NegBackward0>)\n",
            "Time 403.56213760375977 loss tensor(2.2620, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0232, grad_fn=<NegBackward0>)\n",
            "Time 404.5187683105469 loss tensor(2.2617, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1133, grad_fn=<NegBackward0>)\n",
            "Time 405.5070571899414 loss tensor(2.2618, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0083, grad_fn=<NegBackward0>)\n",
            "Time 406.49455308914185 loss tensor(2.2616, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0403, grad_fn=<NegBackward0>)\n",
            "Time 407.462149143219 loss tensor(2.2616, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.3845, grad_fn=<NegBackward0>)\n",
            "Time 408.4377489089966 loss tensor(2.2616, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.2068, grad_fn=<NegBackward0>)\n",
            "Time 409.5607590675354 loss tensor(2.2620, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.4148, grad_fn=<NegBackward0>)\n",
            "Time 410.89290404319763 loss tensor(2.2609, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0160, grad_fn=<NegBackward0>)\n",
            "Time 411.98876905441284 loss tensor(2.2606, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0763, grad_fn=<NegBackward0>)\n",
            "Time 412.8626289367676 loss tensor(2.2606, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0040, grad_fn=<NegBackward0>)\n",
            "Time 413.86079621315 loss tensor(2.2604, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.4052, grad_fn=<NegBackward0>)\n",
            "Time 414.76450204849243 loss tensor(2.2604, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.1157, grad_fn=<NegBackward0>)\n",
            "Time 415.72724175453186 loss tensor(2.2522, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.3199, grad_fn=<NegBackward0>)\n",
            "Time 416.72237515449524 loss tensor(2.2520, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.1692, grad_fn=<NegBackward0>)\n",
            "Time 417.704021692276 loss tensor(2.2502, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.5676, grad_fn=<NegBackward0>)\n",
            "Time 418.6620719432831 loss tensor(2.2499, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.3412, grad_fn=<NegBackward0>)\n",
            "Time 419.67047142982483 loss tensor(2.2428, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.5299, grad_fn=<NegBackward0>)\n",
            "Time 420.6287226676941 loss tensor(2.2408, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.5137, grad_fn=<NegBackward0>)\n",
            "Time 421.48094367980957 loss tensor(2.2266, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.7580, grad_fn=<NegBackward0>)\n",
            "Time 422.81040811538696 loss tensor(2.2208, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.0244, grad_fn=<NegBackward0>)\n",
            "Time 424.1705479621887 loss tensor(2.2215, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.3571, grad_fn=<NegBackward0>)\n",
            "Time 425.0030677318573 loss tensor(2.2215, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.5950, grad_fn=<NegBackward0>)\n",
            "Time 425.97009015083313 loss tensor(2.2184, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.8593, grad_fn=<NegBackward0>)\n",
            "Time 426.84525322914124 loss tensor(2.2182, grad_fn=<AliasBackward0>)\n",
            "dt tensor(1.4109, grad_fn=<NegBackward0>)\n",
            "Time 427.7466185092926 loss tensor(2.2083, grad_fn=<AliasBackward0>)\n",
            "dt tensor(2.0431, grad_fn=<NegBackward0>)\n",
            "Time 428.7114474773407 loss tensor(2.4340, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-0.7419, grad_fn=<NegBackward0>)\n",
            "Time 429.681973695755 loss tensor(2.4029, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-3.0054, grad_fn=<NegBackward0>)\n",
            "Time 430.68067240715027 loss tensor(2.4560, grad_fn=<AliasBackward0>)\n",
            "dt tensor(0.0605, grad_fn=<NegBackward0>)\n",
            "Time 431.5920307636261 loss tensor(3.0541, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-1.8308, grad_fn=<NegBackward0>)\n",
            "Time 432.4337410926819 loss tensor(3.0540, grad_fn=<AliasBackward0>)\n",
            "dt tensor(2.7464, grad_fn=<NegBackward0>)\n",
            "Time 433.3035480976105 loss tensor(3.7416, grad_fn=<AliasBackward0>)\n",
            "dt tensor(-37.7164, grad_fn=<NegBackward0>)\n",
            "Time 434.3560211658478 loss tensor(5.6352, grad_fn=<AliasBackward0>)\n",
            "dt tensor(nan, grad_fn=<NegBackward0>)\n",
            "Time 435.6251962184906 loss tensor(inf, grad_fn=<AliasBackward0>)\n",
            "dt tensor(nan, grad_fn=<NegBackward0>)\n",
            "Time 436.8563754558563 loss tensor(nan, grad_fn=<AliasBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "\n",
        "  cnn.train()\n",
        "  for i in range(1):\n",
        "    xb, yb = next(iter(train_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    conv1w, conv1b, conv2w, conv2b, conv3w, conv3b, conv4w, conv4b, fc1w, fc1b, fc2w, fc2b = train_fwd_gradient(xb, yb)\n",
        "    cnn = SimpleCNN(1, conv1w, conv1b, conv2w, conv2b, conv3w, conv3b, conv4w, conv4b, fc1w, fc1b, fc2w, fc2b).to(device)\n",
        "\n",
        "    #Update cnn parameters\n",
        "    #Recalculate ypred and loss\n",
        "    #MIRAR NN_LAB_LOGISITC_REGRESSION\n",
        "    #CALCULAR G(THETA) QUE ES EL GRADIENTE Y APLICARLO A LOS PARAMETROS DEL CNN, LOS WEIGHTS"
      ],
      "metadata": {
        "id": "V3k3eeKL_TPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}