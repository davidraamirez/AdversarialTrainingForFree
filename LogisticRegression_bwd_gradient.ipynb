{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidraamirez/GradientWithoutBackpropagation/blob/main/LogisticRegression_bwd_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import the necessary packages**\n",
        "\n"
      ],
      "metadata": {
        "id": "x7iZpPFrGWhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rkQFFcg0HXnN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision"
      ],
      "metadata": {
        "id": "ShzuBabDHhVe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "A61m0FPpGijm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time "
      ],
      "metadata": {
        "id": "ZHyH8jgfGmnf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "14D3LUb1KU8z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading and preprocessing the data**"
      ],
      "metadata": {
        "id": "G-a43m8xGoJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "penguins = tfds.load('penguins', as_supervised=True, split='train')"
      ],
      "metadata": {
        "id": "ml7IdhKkHjfC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = penguins.batch(500).get_single_element()\n",
        "X, y = X.numpy(), y.numpy()"
      ],
      "metadata": {
        "id": "c2BMFvK7HoFd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, stratify=y)"
      ],
      "metadata": {
        "id": "bDM9PTN0HrKa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = torch.from_numpy(Xtrain).float()\n",
        "Xtest = torch.from_numpy(Xtest).float()"
      ],
      "metadata": {
        "id": "JLGZsa5pHtTD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain = torch.from_numpy(ytrain).long()\n",
        "ytest = torch.from_numpy(ytest).long()"
      ],
      "metadata": {
        "id": "pubjWXD8Hvva"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Logistic Regression**"
      ],
      "metadata": {
        "id": "9y_Sd0mlG27D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLogisticRegression(nn.Module):\n",
        "  def __init__(self, input_size, w, b):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(w)\n",
        "    self.bias = nn.Parameter(b)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.reshape(1, -1)\n",
        "    return torch.softmax(x@self.weight + self.bias, 1)"
      ],
      "metadata": {
        "id": "YjCVZwwrH3ah"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We check if CUDA is available.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pv1yTiJH7JV",
        "outputId": "9ec0f677-955a-4d32-a337-2206afb6da2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize the parameters**"
      ],
      "metadata": {
        "id": "dUvft9uYG_Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We initialize the parameters randomly and the model with an input size\n",
        "w = torch.randn((4, 3), requires_grad=True)\n",
        "b = torch.randn((3, ), requires_grad=True)\n",
        "LG = SimpleLogisticRegression(4, w, b).to(device)"
      ],
      "metadata": {
        "id": "l2X3STJaH9aY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We try our model with the first example\n",
        "print(LG(Xtrain[0].to(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbykYgnuIAWE",
        "outputId": "5832411c-febb-4478-a16b-39fecba41c71"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2875, 0.1945, 0.5180]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate predictions"
      ],
      "metadata": {
        "id": "-nxjcQ7-HQh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(x,w,b):\n",
        "  ypred=torch.randn((x.shape[0],3))\n",
        "  for j in range (x.shape[0]):\n",
        "    xj = x[j].reshape(1, -1)\n",
        "    ypred[j]=torch.softmax(xj@w+b,1)\n",
        "  return ypred"
      ],
      "metadata": {
        "id": "2OCd-kbfLV5h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred=pred(Xtrain,w,b)"
      ],
      "metadata": {
        "id": "SzBH4FuVNabv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define accuracy**"
      ],
      "metadata": {
        "id": "1g51kfRQHWSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(ytrue, ypred):\n",
        "  return (ypred.argmax(1) == ytrue).float().mean()"
      ],
      "metadata": {
        "id": "_LQ7YyETIB7n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average accuracy at initialization is 33% (random guessing).\n",
        "accuracy(ytrain.to(device),ypred.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bGfj-mfIFwZ",
        "outputId": "825c74c9-1b72-47c6-dba3-397e4477e149"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1880)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define cross entropy**"
      ],
      "metadata": {
        "id": "hiEzUJ_GHmQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(ytrue,ypred):\n",
        "  return - ypred[torch.arange(0, ypred.shape[0]), ytrue].log().mean()"
      ],
      "metadata": {
        "id": "1MVXQ3PTILFk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_entropy(ytrain.to(device),ypred.to(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMDzjWsUK6YG",
        "outputId": "ba5d6aeb-e916-4a5e-9b09-542a946fbc54"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.5727, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train and evaluate the network**"
      ],
      "metadata": {
        "id": "sCrAi6M2H3wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bwd_gradient(x,y):\n",
        "\n",
        "  x,y=x.to(device),y.to(device)\n",
        "\n",
        "  losses = [] # Vector with the cross entropy values of test set\n",
        "  accuracies = [] # Vector with the accuracy values of test set\n",
        "  errors=[] # Vector with the number of misclassification of the test set\n",
        "\n",
        "  l_rate0 = 0.2 # Learning rate used \n",
        "\n",
        "  # Initialize the parameters\n",
        "  w = torch.randn((4, 3), requires_grad=True)\n",
        "  b = torch.randn((3, ), requires_grad=True)\n",
        "\n",
        "  w, b = w.to(device), b.to(device)\n",
        "\n",
        "  ypred=pred(x,w,b)\n",
        "  ypred = ypred.to(device)\n",
        "  loss = cross_entropy(y,ypred) # Loss function\n",
        "\n",
        "  # Calculate the start time \n",
        "  t=0\n",
        "  it=0\n",
        "  t0=time.time()\n",
        "  print('Time', t, 'loss', loss)\n",
        "\n",
        "  while (loss>0.3): \n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # Apply gradients \n",
        "      w -= l_rate0*w.grad\n",
        "      b -= l_rate0*b.grad\n",
        "\n",
        "      # Gradients are accumulated: we need to zero them out before the next iteration.\n",
        "      w.grad.zero_()\n",
        "      b.grad.zero_()\n",
        "    \n",
        "    # We calculate the number of misclassification of the test set with the updated model and we add to the errors vector\n",
        "    LG = SimpleLogisticRegression(4, w, b).to(device)\n",
        "    ypredT = torch.randn(Xtest.size(0),3)\n",
        "    error=0\n",
        "    for i in range (Xtest.size(0)):\n",
        "      ypredT[i]=LG(Xtest[i].to(device))\n",
        "      if (LG(Xtest[i].to(device)).argmax(1)- ytest[i])!=0:\n",
        "        error = error+ 1\n",
        "    errors.append(error)\n",
        "    ypredT = ypredT.to(device)\n",
        "\n",
        "    ypred=pred(x,w,b)\n",
        "    \n",
        "    # We calculate the accuracy of the test set with the updated model and we add to the accuracy vector\n",
        "    accuracies.append(accuracy(ytest,ypredT).item())\n",
        "\n",
        "    # We calculate the cross_entropy of the test set with the updated model and we add to the accuracy vector\n",
        "    loss = cross_entropy(y,ypred.to(device))\n",
        "    lossT = cross_entropy(ytest,ypredT)\n",
        "    losses.append(lossT.detach().item())\n",
        "\n",
        "    #We add the execution time of the iteration\n",
        "    t1=time.time()\n",
        "    t+=t1-t0\n",
        "    t0=t1\n",
        "    it+=1\n",
        "    print('Time', t, 'loss', loss)\n",
        "    \n",
        "  print('Final execution time', t)  \n",
        "  print('Number of iterations', it)\n",
        "  print('Mean execution time of an iteration', t/it)\n",
        "  \n",
        "  return w,b,errors,losses,accuracies\n"
      ],
      "metadata": {
        "id": "cBld3TwvLDl6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b,errors,losses,accuracies = train_bwd_gradient(Xtrain, ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXOhaZSOPSCa",
        "outputId": "104fa543-fc03-4fdc-d593-d7dce66f62f9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time 0 loss tensor(2.5856, grad_fn=<NegBackward0>)\n",
            "Time 0.09977364540100098 loss tensor(2.4084, grad_fn=<NegBackward0>)\n",
            "Time 0.17649340629577637 loss tensor(2.2399, grad_fn=<NegBackward0>)\n",
            "Time 0.28862977027893066 loss tensor(2.0810, grad_fn=<NegBackward0>)\n",
            "Time 0.3781599998474121 loss tensor(1.9326, grad_fn=<NegBackward0>)\n",
            "Time 0.46698522567749023 loss tensor(1.7950, grad_fn=<NegBackward0>)\n",
            "Time 0.6502439975738525 loss tensor(1.6683, grad_fn=<NegBackward0>)\n",
            "Time 0.7762014865875244 loss tensor(1.5522, grad_fn=<NegBackward0>)\n",
            "Time 0.8633406162261963 loss tensor(1.4460, grad_fn=<NegBackward0>)\n",
            "Time 1.054478645324707 loss tensor(1.3491, grad_fn=<NegBackward0>)\n",
            "Time 1.2501678466796875 loss tensor(1.2609, grad_fn=<NegBackward0>)\n",
            "Time 1.576646089553833 loss tensor(1.1808, grad_fn=<NegBackward0>)\n",
            "Time 1.872208833694458 loss tensor(1.1083, grad_fn=<NegBackward0>)\n",
            "Time 2.038992166519165 loss tensor(1.0432, grad_fn=<NegBackward0>)\n",
            "Time 2.2465462684631348 loss tensor(0.9851, grad_fn=<NegBackward0>)\n",
            "Time 2.4279351234436035 loss tensor(0.9336, grad_fn=<NegBackward0>)\n",
            "Time 2.6865601539611816 loss tensor(0.8883, grad_fn=<NegBackward0>)\n",
            "Time 2.947171211242676 loss tensor(0.8488, grad_fn=<NegBackward0>)\n",
            "Time 3.130948543548584 loss tensor(0.8147, grad_fn=<NegBackward0>)\n",
            "Time 3.33152437210083 loss tensor(0.7852, grad_fn=<NegBackward0>)\n",
            "Time 3.4756367206573486 loss tensor(0.7600, grad_fn=<NegBackward0>)\n",
            "Time 3.5851869583129883 loss tensor(0.7385, grad_fn=<NegBackward0>)\n",
            "Time 3.8496756553649902 loss tensor(0.7201, grad_fn=<NegBackward0>)\n",
            "Time 3.9270453453063965 loss tensor(0.7045, grad_fn=<NegBackward0>)\n",
            "Time 3.999396324157715 loss tensor(0.6911, grad_fn=<NegBackward0>)\n",
            "Time 4.072626829147339 loss tensor(0.6797, grad_fn=<NegBackward0>)\n",
            "Time 4.142750024795532 loss tensor(0.6699, grad_fn=<NegBackward0>)\n",
            "Time 4.2260401248931885 loss tensor(0.6614, grad_fn=<NegBackward0>)\n",
            "Time 4.302513599395752 loss tensor(0.6541, grad_fn=<NegBackward0>)\n",
            "Time 4.398444652557373 loss tensor(0.6477, grad_fn=<NegBackward0>)\n",
            "Time 4.5374250411987305 loss tensor(0.6421, grad_fn=<NegBackward0>)\n",
            "Time 4.616701364517212 loss tensor(0.6371, grad_fn=<NegBackward0>)\n",
            "Time 4.7037129402160645 loss tensor(0.6327, grad_fn=<NegBackward0>)\n",
            "Time 4.777059555053711 loss tensor(0.6287, grad_fn=<NegBackward0>)\n",
            "Time 4.859393119812012 loss tensor(0.6251, grad_fn=<NegBackward0>)\n",
            "Time 4.9384307861328125 loss tensor(0.6218, grad_fn=<NegBackward0>)\n",
            "Time 5.010784149169922 loss tensor(0.6187, grad_fn=<NegBackward0>)\n",
            "Time 5.087454319000244 loss tensor(0.6159, grad_fn=<NegBackward0>)\n",
            "Time 5.165315628051758 loss tensor(0.6132, grad_fn=<NegBackward0>)\n",
            "Time 5.248009204864502 loss tensor(0.6107, grad_fn=<NegBackward0>)\n",
            "Time 5.340126276016235 loss tensor(0.6083, grad_fn=<NegBackward0>)\n",
            "Time 5.413348913192749 loss tensor(0.6061, grad_fn=<NegBackward0>)\n",
            "Time 5.512990951538086 loss tensor(0.6039, grad_fn=<NegBackward0>)\n",
            "Time 5.580549955368042 loss tensor(0.6018, grad_fn=<NegBackward0>)\n",
            "Time 5.678105592727661 loss tensor(0.5997, grad_fn=<NegBackward0>)\n",
            "Time 5.803334712982178 loss tensor(0.5977, grad_fn=<NegBackward0>)\n",
            "Time 5.900112152099609 loss tensor(0.5958, grad_fn=<NegBackward0>)\n",
            "Time 5.995453596115112 loss tensor(0.5939, grad_fn=<NegBackward0>)\n",
            "Time 6.07250189781189 loss tensor(0.5920, grad_fn=<NegBackward0>)\n",
            "Time 6.181456804275513 loss tensor(0.5902, grad_fn=<NegBackward0>)\n",
            "Time 6.310654878616333 loss tensor(0.5884, grad_fn=<NegBackward0>)\n",
            "Time 6.423989534378052 loss tensor(0.5866, grad_fn=<NegBackward0>)\n",
            "Time 6.506715774536133 loss tensor(0.5849, grad_fn=<NegBackward0>)\n",
            "Time 6.651017189025879 loss tensor(0.5831, grad_fn=<NegBackward0>)\n",
            "Time 6.721530199050903 loss tensor(0.5814, grad_fn=<NegBackward0>)\n",
            "Time 6.7983458042144775 loss tensor(0.5797, grad_fn=<NegBackward0>)\n",
            "Time 6.878504276275635 loss tensor(0.5781, grad_fn=<NegBackward0>)\n",
            "Time 7.033965349197388 loss tensor(0.5764, grad_fn=<NegBackward0>)\n",
            "Time 7.127259254455566 loss tensor(0.5747, grad_fn=<NegBackward0>)\n",
            "Time 7.24785304069519 loss tensor(0.5731, grad_fn=<NegBackward0>)\n",
            "Time 7.342795133590698 loss tensor(0.5715, grad_fn=<NegBackward0>)\n",
            "Time 7.421460151672363 loss tensor(0.5699, grad_fn=<NegBackward0>)\n",
            "Time 7.502676486968994 loss tensor(0.5683, grad_fn=<NegBackward0>)\n",
            "Time 7.627686977386475 loss tensor(0.5667, grad_fn=<NegBackward0>)\n",
            "Time 7.790161848068237 loss tensor(0.5651, grad_fn=<NegBackward0>)\n",
            "Time 7.86894154548645 loss tensor(0.5635, grad_fn=<NegBackward0>)\n",
            "Time 7.942537307739258 loss tensor(0.5620, grad_fn=<NegBackward0>)\n",
            "Time 8.077796459197998 loss tensor(0.5604, grad_fn=<NegBackward0>)\n",
            "Time 8.211447477340698 loss tensor(0.5589, grad_fn=<NegBackward0>)\n",
            "Time 8.303812265396118 loss tensor(0.5574, grad_fn=<NegBackward0>)\n",
            "Time 8.373846530914307 loss tensor(0.5559, grad_fn=<NegBackward0>)\n",
            "Time 8.4675772190094 loss tensor(0.5544, grad_fn=<NegBackward0>)\n",
            "Time 8.609757423400879 loss tensor(0.5529, grad_fn=<NegBackward0>)\n",
            "Time 8.699000597000122 loss tensor(0.5514, grad_fn=<NegBackward0>)\n",
            "Time 8.804056167602539 loss tensor(0.5499, grad_fn=<NegBackward0>)\n",
            "Time 8.903859853744507 loss tensor(0.5484, grad_fn=<NegBackward0>)\n",
            "Time 8.981369733810425 loss tensor(0.5469, grad_fn=<NegBackward0>)\n",
            "Time 9.070905447006226 loss tensor(0.5455, grad_fn=<NegBackward0>)\n",
            "Time 9.143986225128174 loss tensor(0.5440, grad_fn=<NegBackward0>)\n",
            "Time 9.212851762771606 loss tensor(0.5426, grad_fn=<NegBackward0>)\n",
            "Time 9.301583290100098 loss tensor(0.5412, grad_fn=<NegBackward0>)\n",
            "Time 9.383601903915405 loss tensor(0.5397, grad_fn=<NegBackward0>)\n",
            "Time 9.475296258926392 loss tensor(0.5383, grad_fn=<NegBackward0>)\n",
            "Time 9.560570478439331 loss tensor(0.5369, grad_fn=<NegBackward0>)\n",
            "Time 9.665753841400146 loss tensor(0.5355, grad_fn=<NegBackward0>)\n",
            "Time 9.815075874328613 loss tensor(0.5341, grad_fn=<NegBackward0>)\n",
            "Time 9.909788608551025 loss tensor(0.5328, grad_fn=<NegBackward0>)\n",
            "Time 10.043647766113281 loss tensor(0.5314, grad_fn=<NegBackward0>)\n",
            "Time 10.11941409111023 loss tensor(0.5300, grad_fn=<NegBackward0>)\n",
            "Time 10.202633380889893 loss tensor(0.5286, grad_fn=<NegBackward0>)\n",
            "Time 10.316937923431396 loss tensor(0.5273, grad_fn=<NegBackward0>)\n",
            "Time 10.44285273551941 loss tensor(0.5260, grad_fn=<NegBackward0>)\n",
            "Time 10.538777112960815 loss tensor(0.5246, grad_fn=<NegBackward0>)\n",
            "Time 10.62237548828125 loss tensor(0.5233, grad_fn=<NegBackward0>)\n",
            "Time 10.723328590393066 loss tensor(0.5220, grad_fn=<NegBackward0>)\n",
            "Time 10.81691575050354 loss tensor(0.5206, grad_fn=<NegBackward0>)\n",
            "Time 11.021491765975952 loss tensor(0.5193, grad_fn=<NegBackward0>)\n",
            "Time 11.153176307678223 loss tensor(0.5180, grad_fn=<NegBackward0>)\n",
            "Time 11.263808250427246 loss tensor(0.5167, grad_fn=<NegBackward0>)\n",
            "Time 11.395398378372192 loss tensor(0.5154, grad_fn=<NegBackward0>)\n",
            "Time 11.47930097579956 loss tensor(0.5142, grad_fn=<NegBackward0>)\n",
            "Time 11.556739330291748 loss tensor(0.5129, grad_fn=<NegBackward0>)\n",
            "Time 11.763490676879883 loss tensor(0.5116, grad_fn=<NegBackward0>)\n",
            "Time 11.848722696304321 loss tensor(0.5103, grad_fn=<NegBackward0>)\n",
            "Time 11.927526473999023 loss tensor(0.5091, grad_fn=<NegBackward0>)\n",
            "Time 12.095182418823242 loss tensor(0.5078, grad_fn=<NegBackward0>)\n",
            "Time 12.234196901321411 loss tensor(0.5066, grad_fn=<NegBackward0>)\n",
            "Time 12.390139818191528 loss tensor(0.5054, grad_fn=<NegBackward0>)\n",
            "Time 12.475543737411499 loss tensor(0.5041, grad_fn=<NegBackward0>)\n",
            "Time 12.615684270858765 loss tensor(0.5029, grad_fn=<NegBackward0>)\n",
            "Time 12.707673788070679 loss tensor(0.5017, grad_fn=<NegBackward0>)\n",
            "Time 12.842785596847534 loss tensor(0.5005, grad_fn=<NegBackward0>)\n",
            "Time 12.920985698699951 loss tensor(0.4993, grad_fn=<NegBackward0>)\n",
            "Time 13.040156364440918 loss tensor(0.4981, grad_fn=<NegBackward0>)\n",
            "Time 13.180653810501099 loss tensor(0.4969, grad_fn=<NegBackward0>)\n",
            "Time 13.315822124481201 loss tensor(0.4957, grad_fn=<NegBackward0>)\n",
            "Time 13.538771867752075 loss tensor(0.4945, grad_fn=<NegBackward0>)\n",
            "Time 13.775562286376953 loss tensor(0.4933, grad_fn=<NegBackward0>)\n",
            "Time 13.89336609840393 loss tensor(0.4922, grad_fn=<NegBackward0>)\n",
            "Time 14.024656295776367 loss tensor(0.4910, grad_fn=<NegBackward0>)\n",
            "Time 14.261656761169434 loss tensor(0.4898, grad_fn=<NegBackward0>)\n",
            "Time 14.484952688217163 loss tensor(0.4887, grad_fn=<NegBackward0>)\n",
            "Time 14.661888122558594 loss tensor(0.4875, grad_fn=<NegBackward0>)\n",
            "Time 14.93819785118103 loss tensor(0.4864, grad_fn=<NegBackward0>)\n",
            "Time 15.19823670387268 loss tensor(0.4853, grad_fn=<NegBackward0>)\n",
            "Time 15.405003309249878 loss tensor(0.4841, grad_fn=<NegBackward0>)\n",
            "Time 15.613747119903564 loss tensor(0.4830, grad_fn=<NegBackward0>)\n",
            "Time 15.790689945220947 loss tensor(0.4819, grad_fn=<NegBackward0>)\n",
            "Time 16.042505979537964 loss tensor(0.4808, grad_fn=<NegBackward0>)\n",
            "Time 16.27906346321106 loss tensor(0.4797, grad_fn=<NegBackward0>)\n",
            "Time 16.44210696220398 loss tensor(0.4786, grad_fn=<NegBackward0>)\n",
            "Time 16.568644762039185 loss tensor(0.4775, grad_fn=<NegBackward0>)\n",
            "Time 16.68952512741089 loss tensor(0.4764, grad_fn=<NegBackward0>)\n",
            "Time 16.75930666923523 loss tensor(0.4753, grad_fn=<NegBackward0>)\n",
            "Time 16.848760843276978 loss tensor(0.4742, grad_fn=<NegBackward0>)\n",
            "Time 16.929121732711792 loss tensor(0.4731, grad_fn=<NegBackward0>)\n",
            "Time 17.02328586578369 loss tensor(0.4721, grad_fn=<NegBackward0>)\n",
            "Time 17.123838663101196 loss tensor(0.4710, grad_fn=<NegBackward0>)\n",
            "Time 17.226733684539795 loss tensor(0.4699, grad_fn=<NegBackward0>)\n",
            "Time 17.316288948059082 loss tensor(0.4689, grad_fn=<NegBackward0>)\n",
            "Time 17.39140558242798 loss tensor(0.4678, grad_fn=<NegBackward0>)\n",
            "Time 17.463555812835693 loss tensor(0.4668, grad_fn=<NegBackward0>)\n",
            "Time 17.548374891281128 loss tensor(0.4657, grad_fn=<NegBackward0>)\n",
            "Time 17.635340929031372 loss tensor(0.4647, grad_fn=<NegBackward0>)\n",
            "Time 17.74280619621277 loss tensor(0.4637, grad_fn=<NegBackward0>)\n",
            "Time 17.83056616783142 loss tensor(0.4626, grad_fn=<NegBackward0>)\n",
            "Time 17.989341020584106 loss tensor(0.4616, grad_fn=<NegBackward0>)\n",
            "Time 18.14131474494934 loss tensor(0.4606, grad_fn=<NegBackward0>)\n",
            "Time 18.26755952835083 loss tensor(0.4596, grad_fn=<NegBackward0>)\n",
            "Time 18.491527318954468 loss tensor(0.4586, grad_fn=<NegBackward0>)\n",
            "Time 18.601642847061157 loss tensor(0.4576, grad_fn=<NegBackward0>)\n",
            "Time 18.69056534767151 loss tensor(0.4566, grad_fn=<NegBackward0>)\n",
            "Time 18.83293056488037 loss tensor(0.4556, grad_fn=<NegBackward0>)\n",
            "Time 18.952916860580444 loss tensor(0.4546, grad_fn=<NegBackward0>)\n",
            "Time 19.070844411849976 loss tensor(0.4536, grad_fn=<NegBackward0>)\n",
            "Time 19.31740665435791 loss tensor(0.4526, grad_fn=<NegBackward0>)\n",
            "Time 19.519893169403076 loss tensor(0.4517, grad_fn=<NegBackward0>)\n",
            "Time 19.740712881088257 loss tensor(0.4507, grad_fn=<NegBackward0>)\n",
            "Time 19.918186902999878 loss tensor(0.4497, grad_fn=<NegBackward0>)\n",
            "Time 19.976003170013428 loss tensor(0.4487, grad_fn=<NegBackward0>)\n",
            "Time 20.02881360054016 loss tensor(0.4478, grad_fn=<NegBackward0>)\n",
            "Time 20.073960065841675 loss tensor(0.4468, grad_fn=<NegBackward0>)\n",
            "Time 20.152168035507202 loss tensor(0.4459, grad_fn=<NegBackward0>)\n",
            "Time 20.21342134475708 loss tensor(0.4449, grad_fn=<NegBackward0>)\n",
            "Time 20.259702920913696 loss tensor(0.4440, grad_fn=<NegBackward0>)\n",
            "Time 20.306478023529053 loss tensor(0.4431, grad_fn=<NegBackward0>)\n",
            "Time 20.351425170898438 loss tensor(0.4421, grad_fn=<NegBackward0>)\n",
            "Time 20.397406339645386 loss tensor(0.4412, grad_fn=<NegBackward0>)\n",
            "Time 20.44864583015442 loss tensor(0.4403, grad_fn=<NegBackward0>)\n",
            "Time 20.49547839164734 loss tensor(0.4393, grad_fn=<NegBackward0>)\n",
            "Time 20.548134565353394 loss tensor(0.4384, grad_fn=<NegBackward0>)\n",
            "Time 20.59867572784424 loss tensor(0.4375, grad_fn=<NegBackward0>)\n",
            "Time 20.645514726638794 loss tensor(0.4366, grad_fn=<NegBackward0>)\n",
            "Time 20.697436332702637 loss tensor(0.4357, grad_fn=<NegBackward0>)\n",
            "Time 20.752134323120117 loss tensor(0.4348, grad_fn=<NegBackward0>)\n",
            "Time 20.799197673797607 loss tensor(0.4339, grad_fn=<NegBackward0>)\n",
            "Time 20.844013690948486 loss tensor(0.4330, grad_fn=<NegBackward0>)\n",
            "Time 20.88792061805725 loss tensor(0.4321, grad_fn=<NegBackward0>)\n",
            "Time 20.940298318862915 loss tensor(0.4312, grad_fn=<NegBackward0>)\n",
            "Time 20.98414134979248 loss tensor(0.4304, grad_fn=<NegBackward0>)\n",
            "Time 21.03702402114868 loss tensor(0.4295, grad_fn=<NegBackward0>)\n",
            "Time 21.081716060638428 loss tensor(0.4286, grad_fn=<NegBackward0>)\n",
            "Time 21.125315189361572 loss tensor(0.4277, grad_fn=<NegBackward0>)\n",
            "Time 21.184118509292603 loss tensor(0.4269, grad_fn=<NegBackward0>)\n",
            "Time 21.231763124465942 loss tensor(0.4260, grad_fn=<NegBackward0>)\n",
            "Time 21.277117490768433 loss tensor(0.4251, grad_fn=<NegBackward0>)\n",
            "Time 21.321752548217773 loss tensor(0.4243, grad_fn=<NegBackward0>)\n",
            "Time 21.370599031448364 loss tensor(0.4234, grad_fn=<NegBackward0>)\n",
            "Time 21.418590545654297 loss tensor(0.4226, grad_fn=<NegBackward0>)\n",
            "Time 21.466525077819824 loss tensor(0.4217, grad_fn=<NegBackward0>)\n",
            "Time 21.511531591415405 loss tensor(0.4209, grad_fn=<NegBackward0>)\n",
            "Time 21.5563907623291 loss tensor(0.4201, grad_fn=<NegBackward0>)\n",
            "Time 21.60061502456665 loss tensor(0.4192, grad_fn=<NegBackward0>)\n",
            "Time 21.650395154953003 loss tensor(0.4184, grad_fn=<NegBackward0>)\n",
            "Time 21.70172095298767 loss tensor(0.4176, grad_fn=<NegBackward0>)\n",
            "Time 21.752265214920044 loss tensor(0.4167, grad_fn=<NegBackward0>)\n",
            "Time 21.80028009414673 loss tensor(0.4159, grad_fn=<NegBackward0>)\n",
            "Time 21.844354152679443 loss tensor(0.4151, grad_fn=<NegBackward0>)\n",
            "Time 21.892279624938965 loss tensor(0.4143, grad_fn=<NegBackward0>)\n",
            "Time 21.94112253189087 loss tensor(0.4135, grad_fn=<NegBackward0>)\n",
            "Time 21.985404014587402 loss tensor(0.4127, grad_fn=<NegBackward0>)\n",
            "Time 22.032403230667114 loss tensor(0.4119, grad_fn=<NegBackward0>)\n",
            "Time 22.080105781555176 loss tensor(0.4111, grad_fn=<NegBackward0>)\n",
            "Time 22.128459453582764 loss tensor(0.4103, grad_fn=<NegBackward0>)\n",
            "Time 22.177666902542114 loss tensor(0.4095, grad_fn=<NegBackward0>)\n",
            "Time 22.249956846237183 loss tensor(0.4087, grad_fn=<NegBackward0>)\n",
            "Time 22.294666528701782 loss tensor(0.4079, grad_fn=<NegBackward0>)\n",
            "Time 22.34351873397827 loss tensor(0.4071, grad_fn=<NegBackward0>)\n",
            "Time 22.388955116271973 loss tensor(0.4063, grad_fn=<NegBackward0>)\n",
            "Time 22.433533668518066 loss tensor(0.4056, grad_fn=<NegBackward0>)\n",
            "Time 22.48652935028076 loss tensor(0.4048, grad_fn=<NegBackward0>)\n",
            "Time 22.550376892089844 loss tensor(0.4040, grad_fn=<NegBackward0>)\n",
            "Time 22.610962629318237 loss tensor(0.4032, grad_fn=<NegBackward0>)\n",
            "Time 22.66569685935974 loss tensor(0.4025, grad_fn=<NegBackward0>)\n",
            "Time 22.712724924087524 loss tensor(0.4017, grad_fn=<NegBackward0>)\n",
            "Time 22.7643084526062 loss tensor(0.4010, grad_fn=<NegBackward0>)\n",
            "Time 22.813803672790527 loss tensor(0.4002, grad_fn=<NegBackward0>)\n",
            "Time 22.85773730278015 loss tensor(0.3994, grad_fn=<NegBackward0>)\n",
            "Time 22.90142035484314 loss tensor(0.3987, grad_fn=<NegBackward0>)\n",
            "Time 22.947970628738403 loss tensor(0.3980, grad_fn=<NegBackward0>)\n",
            "Time 23.000636339187622 loss tensor(0.3972, grad_fn=<NegBackward0>)\n",
            "Time 23.044004678726196 loss tensor(0.3965, grad_fn=<NegBackward0>)\n",
            "Time 23.088014125823975 loss tensor(0.3957, grad_fn=<NegBackward0>)\n",
            "Time 23.13977861404419 loss tensor(0.3950, grad_fn=<NegBackward0>)\n",
            "Time 23.184925317764282 loss tensor(0.3943, grad_fn=<NegBackward0>)\n",
            "Time 23.24560856819153 loss tensor(0.3935, grad_fn=<NegBackward0>)\n",
            "Time 23.290802478790283 loss tensor(0.3928, grad_fn=<NegBackward0>)\n",
            "Time 23.334157466888428 loss tensor(0.3921, grad_fn=<NegBackward0>)\n",
            "Time 23.377015829086304 loss tensor(0.3914, grad_fn=<NegBackward0>)\n",
            "Time 23.42239499092102 loss tensor(0.3906, grad_fn=<NegBackward0>)\n",
            "Time 23.475743293762207 loss tensor(0.3899, grad_fn=<NegBackward0>)\n",
            "Time 23.519481897354126 loss tensor(0.3892, grad_fn=<NegBackward0>)\n",
            "Time 23.566070556640625 loss tensor(0.3885, grad_fn=<NegBackward0>)\n",
            "Time 23.612967252731323 loss tensor(0.3878, grad_fn=<NegBackward0>)\n",
            "Time 23.657177209854126 loss tensor(0.3871, grad_fn=<NegBackward0>)\n",
            "Time 23.70530366897583 loss tensor(0.3864, grad_fn=<NegBackward0>)\n",
            "Time 23.75325608253479 loss tensor(0.3857, grad_fn=<NegBackward0>)\n",
            "Time 23.80272340774536 loss tensor(0.3850, grad_fn=<NegBackward0>)\n",
            "Time 23.84676766395569 loss tensor(0.3843, grad_fn=<NegBackward0>)\n",
            "Time 23.88980531692505 loss tensor(0.3836, grad_fn=<NegBackward0>)\n",
            "Time 23.94011425971985 loss tensor(0.3829, grad_fn=<NegBackward0>)\n",
            "Time 23.983546018600464 loss tensor(0.3822, grad_fn=<NegBackward0>)\n",
            "Time 24.03037714958191 loss tensor(0.3816, grad_fn=<NegBackward0>)\n",
            "Time 24.073126077651978 loss tensor(0.3809, grad_fn=<NegBackward0>)\n",
            "Time 24.119749784469604 loss tensor(0.3802, grad_fn=<NegBackward0>)\n",
            "Time 24.168919801712036 loss tensor(0.3795, grad_fn=<NegBackward0>)\n",
            "Time 24.212302446365356 loss tensor(0.3789, grad_fn=<NegBackward0>)\n",
            "Time 24.26641273498535 loss tensor(0.3782, grad_fn=<NegBackward0>)\n",
            "Time 24.310065984725952 loss tensor(0.3775, grad_fn=<NegBackward0>)\n",
            "Time 24.355044841766357 loss tensor(0.3769, grad_fn=<NegBackward0>)\n",
            "Time 24.405967712402344 loss tensor(0.3762, grad_fn=<NegBackward0>)\n",
            "Time 24.45239758491516 loss tensor(0.3755, grad_fn=<NegBackward0>)\n",
            "Time 24.497082710266113 loss tensor(0.3749, grad_fn=<NegBackward0>)\n",
            "Time 24.546059131622314 loss tensor(0.3742, grad_fn=<NegBackward0>)\n",
            "Time 24.58950161933899 loss tensor(0.3736, grad_fn=<NegBackward0>)\n",
            "Time 24.638426542282104 loss tensor(0.3729, grad_fn=<NegBackward0>)\n",
            "Time 24.68317151069641 loss tensor(0.3723, grad_fn=<NegBackward0>)\n",
            "Time 24.728419065475464 loss tensor(0.3716, grad_fn=<NegBackward0>)\n",
            "Time 24.774401426315308 loss tensor(0.3710, grad_fn=<NegBackward0>)\n",
            "Time 24.821038961410522 loss tensor(0.3703, grad_fn=<NegBackward0>)\n",
            "Time 24.87373924255371 loss tensor(0.3697, grad_fn=<NegBackward0>)\n",
            "Time 24.91754961013794 loss tensor(0.3691, grad_fn=<NegBackward0>)\n",
            "Time 24.963340044021606 loss tensor(0.3684, grad_fn=<NegBackward0>)\n",
            "Time 25.01091432571411 loss tensor(0.3678, grad_fn=<NegBackward0>)\n",
            "Time 25.055364847183228 loss tensor(0.3672, grad_fn=<NegBackward0>)\n",
            "Time 25.10496950149536 loss tensor(0.3666, grad_fn=<NegBackward0>)\n",
            "Time 25.149258852005005 loss tensor(0.3659, grad_fn=<NegBackward0>)\n",
            "Time 25.1921865940094 loss tensor(0.3653, grad_fn=<NegBackward0>)\n",
            "Time 25.248344659805298 loss tensor(0.3647, grad_fn=<NegBackward0>)\n",
            "Time 25.30229687690735 loss tensor(0.3641, grad_fn=<NegBackward0>)\n",
            "Time 25.35188102722168 loss tensor(0.3635, grad_fn=<NegBackward0>)\n",
            "Time 25.397599458694458 loss tensor(0.3628, grad_fn=<NegBackward0>)\n",
            "Time 25.441649913787842 loss tensor(0.3622, grad_fn=<NegBackward0>)\n",
            "Time 25.486091136932373 loss tensor(0.3616, grad_fn=<NegBackward0>)\n",
            "Time 25.531556129455566 loss tensor(0.3610, grad_fn=<NegBackward0>)\n",
            "Time 25.58038592338562 loss tensor(0.3604, grad_fn=<NegBackward0>)\n",
            "Time 25.62443447113037 loss tensor(0.3598, grad_fn=<NegBackward0>)\n",
            "Time 25.67627501487732 loss tensor(0.3592, grad_fn=<NegBackward0>)\n",
            "Time 25.72407603263855 loss tensor(0.3586, grad_fn=<NegBackward0>)\n",
            "Time 25.770116329193115 loss tensor(0.3580, grad_fn=<NegBackward0>)\n",
            "Time 25.82014012336731 loss tensor(0.3574, grad_fn=<NegBackward0>)\n",
            "Time 25.864752054214478 loss tensor(0.3568, grad_fn=<NegBackward0>)\n",
            "Time 25.908323764801025 loss tensor(0.3563, grad_fn=<NegBackward0>)\n",
            "Time 25.954166889190674 loss tensor(0.3557, grad_fn=<NegBackward0>)\n",
            "Time 26.00053381919861 loss tensor(0.3551, grad_fn=<NegBackward0>)\n",
            "Time 26.05302596092224 loss tensor(0.3545, grad_fn=<NegBackward0>)\n",
            "Time 26.097413539886475 loss tensor(0.3539, grad_fn=<NegBackward0>)\n",
            "Time 26.142874717712402 loss tensor(0.3534, grad_fn=<NegBackward0>)\n",
            "Time 26.18636441230774 loss tensor(0.3528, grad_fn=<NegBackward0>)\n",
            "Time 26.23017978668213 loss tensor(0.3522, grad_fn=<NegBackward0>)\n",
            "Time 26.285594701766968 loss tensor(0.3516, grad_fn=<NegBackward0>)\n",
            "Time 26.340155124664307 loss tensor(0.3511, grad_fn=<NegBackward0>)\n",
            "Time 26.383241415023804 loss tensor(0.3505, grad_fn=<NegBackward0>)\n",
            "Time 26.42734456062317 loss tensor(0.3499, grad_fn=<NegBackward0>)\n",
            "Time 26.471624612808228 loss tensor(0.3494, grad_fn=<NegBackward0>)\n",
            "Time 26.54194664955139 loss tensor(0.3488, grad_fn=<NegBackward0>)\n",
            "Time 26.61450958251953 loss tensor(0.3482, grad_fn=<NegBackward0>)\n",
            "Time 26.682252883911133 loss tensor(0.3477, grad_fn=<NegBackward0>)\n",
            "Time 26.756964921951294 loss tensor(0.3471, grad_fn=<NegBackward0>)\n",
            "Time 26.833454608917236 loss tensor(0.3466, grad_fn=<NegBackward0>)\n",
            "Time 26.899282217025757 loss tensor(0.3460, grad_fn=<NegBackward0>)\n",
            "Time 26.9696102142334 loss tensor(0.3455, grad_fn=<NegBackward0>)\n",
            "Time 27.04062509536743 loss tensor(0.3449, grad_fn=<NegBackward0>)\n",
            "Time 27.105076551437378 loss tensor(0.3444, grad_fn=<NegBackward0>)\n",
            "Time 27.19047498703003 loss tensor(0.3438, grad_fn=<NegBackward0>)\n",
            "Time 27.279435634613037 loss tensor(0.3433, grad_fn=<NegBackward0>)\n",
            "Time 27.357194662094116 loss tensor(0.3428, grad_fn=<NegBackward0>)\n",
            "Time 27.430100917816162 loss tensor(0.3422, grad_fn=<NegBackward0>)\n",
            "Time 27.495871782302856 loss tensor(0.3417, grad_fn=<NegBackward0>)\n",
            "Time 27.55893301963806 loss tensor(0.3412, grad_fn=<NegBackward0>)\n",
            "Time 27.624323844909668 loss tensor(0.3406, grad_fn=<NegBackward0>)\n",
            "Time 27.703641414642334 loss tensor(0.3401, grad_fn=<NegBackward0>)\n",
            "Time 27.77106809616089 loss tensor(0.3396, grad_fn=<NegBackward0>)\n",
            "Time 27.840168952941895 loss tensor(0.3390, grad_fn=<NegBackward0>)\n",
            "Time 27.9086811542511 loss tensor(0.3385, grad_fn=<NegBackward0>)\n",
            "Time 27.9844708442688 loss tensor(0.3380, grad_fn=<NegBackward0>)\n",
            "Time 28.05193328857422 loss tensor(0.3375, grad_fn=<NegBackward0>)\n",
            "Time 28.122511386871338 loss tensor(0.3369, grad_fn=<NegBackward0>)\n",
            "Time 28.187848806381226 loss tensor(0.3364, grad_fn=<NegBackward0>)\n",
            "Time 28.251582622528076 loss tensor(0.3359, grad_fn=<NegBackward0>)\n",
            "Time 28.316907167434692 loss tensor(0.3354, grad_fn=<NegBackward0>)\n",
            "Time 28.392883777618408 loss tensor(0.3349, grad_fn=<NegBackward0>)\n",
            "Time 28.46293568611145 loss tensor(0.3344, grad_fn=<NegBackward0>)\n",
            "Time 28.53066372871399 loss tensor(0.3339, grad_fn=<NegBackward0>)\n",
            "Time 28.602166891098022 loss tensor(0.3333, grad_fn=<NegBackward0>)\n",
            "Time 28.67200469970703 loss tensor(0.3328, grad_fn=<NegBackward0>)\n",
            "Time 28.74151062965393 loss tensor(0.3323, grad_fn=<NegBackward0>)\n",
            "Time 28.811784744262695 loss tensor(0.3318, grad_fn=<NegBackward0>)\n",
            "Time 28.88291621208191 loss tensor(0.3313, grad_fn=<NegBackward0>)\n",
            "Time 28.95342230796814 loss tensor(0.3308, grad_fn=<NegBackward0>)\n",
            "Time 29.00198459625244 loss tensor(0.3303, grad_fn=<NegBackward0>)\n",
            "Time 29.051055192947388 loss tensor(0.3298, grad_fn=<NegBackward0>)\n",
            "Time 29.09635591506958 loss tensor(0.3293, grad_fn=<NegBackward0>)\n",
            "Time 29.141462802886963 loss tensor(0.3289, grad_fn=<NegBackward0>)\n",
            "Time 29.18501901626587 loss tensor(0.3284, grad_fn=<NegBackward0>)\n",
            "Time 29.22887635231018 loss tensor(0.3279, grad_fn=<NegBackward0>)\n",
            "Time 29.2823269367218 loss tensor(0.3274, grad_fn=<NegBackward0>)\n",
            "Time 29.326789140701294 loss tensor(0.3269, grad_fn=<NegBackward0>)\n",
            "Time 29.36997389793396 loss tensor(0.3264, grad_fn=<NegBackward0>)\n",
            "Time 29.412540674209595 loss tensor(0.3259, grad_fn=<NegBackward0>)\n",
            "Time 29.46672534942627 loss tensor(0.3255, grad_fn=<NegBackward0>)\n",
            "Time 29.51447606086731 loss tensor(0.3250, grad_fn=<NegBackward0>)\n",
            "Time 29.557836771011353 loss tensor(0.3245, grad_fn=<NegBackward0>)\n",
            "Time 29.609543800354004 loss tensor(0.3240, grad_fn=<NegBackward0>)\n",
            "Time 29.655290365219116 loss tensor(0.3235, grad_fn=<NegBackward0>)\n",
            "Time 29.698389530181885 loss tensor(0.3231, grad_fn=<NegBackward0>)\n",
            "Time 29.74817705154419 loss tensor(0.3226, grad_fn=<NegBackward0>)\n",
            "Time 29.792615175247192 loss tensor(0.3221, grad_fn=<NegBackward0>)\n",
            "Time 29.839126348495483 loss tensor(0.3217, grad_fn=<NegBackward0>)\n",
            "Time 29.883949995040894 loss tensor(0.3212, grad_fn=<NegBackward0>)\n",
            "Time 29.929720640182495 loss tensor(0.3207, grad_fn=<NegBackward0>)\n",
            "Time 29.980249166488647 loss tensor(0.3203, grad_fn=<NegBackward0>)\n",
            "Time 30.031368732452393 loss tensor(0.3198, grad_fn=<NegBackward0>)\n",
            "Time 30.077710390090942 loss tensor(0.3193, grad_fn=<NegBackward0>)\n",
            "Time 30.121622562408447 loss tensor(0.3189, grad_fn=<NegBackward0>)\n",
            "Time 30.166144371032715 loss tensor(0.3184, grad_fn=<NegBackward0>)\n",
            "Time 30.215033292770386 loss tensor(0.3180, grad_fn=<NegBackward0>)\n",
            "Time 30.259883642196655 loss tensor(0.3175, grad_fn=<NegBackward0>)\n",
            "Time 30.30625081062317 loss tensor(0.3171, grad_fn=<NegBackward0>)\n",
            "Time 30.355663537979126 loss tensor(0.3166, grad_fn=<NegBackward0>)\n",
            "Time 30.40024971961975 loss tensor(0.3162, grad_fn=<NegBackward0>)\n",
            "Time 30.454334020614624 loss tensor(0.3157, grad_fn=<NegBackward0>)\n",
            "Time 30.526090145111084 loss tensor(0.3153, grad_fn=<NegBackward0>)\n",
            "Time 30.572057962417603 loss tensor(0.3148, grad_fn=<NegBackward0>)\n",
            "Time 30.62296414375305 loss tensor(0.3144, grad_fn=<NegBackward0>)\n",
            "Time 30.673433780670166 loss tensor(0.3139, grad_fn=<NegBackward0>)\n",
            "Time 30.718706369400024 loss tensor(0.3135, grad_fn=<NegBackward0>)\n",
            "Time 30.765814542770386 loss tensor(0.3130, grad_fn=<NegBackward0>)\n",
            "Time 30.814436435699463 loss tensor(0.3126, grad_fn=<NegBackward0>)\n",
            "Time 30.86325764656067 loss tensor(0.3122, grad_fn=<NegBackward0>)\n",
            "Time 30.914511919021606 loss tensor(0.3117, grad_fn=<NegBackward0>)\n",
            "Time 30.95927596092224 loss tensor(0.3113, grad_fn=<NegBackward0>)\n",
            "Time 31.010684728622437 loss tensor(0.3109, grad_fn=<NegBackward0>)\n",
            "Time 31.055724620819092 loss tensor(0.3104, grad_fn=<NegBackward0>)\n",
            "Time 31.10080051422119 loss tensor(0.3100, grad_fn=<NegBackward0>)\n",
            "Time 31.15730881690979 loss tensor(0.3096, grad_fn=<NegBackward0>)\n",
            "Time 31.20186686515808 loss tensor(0.3091, grad_fn=<NegBackward0>)\n",
            "Time 31.247884273529053 loss tensor(0.3087, grad_fn=<NegBackward0>)\n",
            "Time 31.29627299308777 loss tensor(0.3083, grad_fn=<NegBackward0>)\n",
            "Time 31.344210147857666 loss tensor(0.3079, grad_fn=<NegBackward0>)\n",
            "Time 31.4020574092865 loss tensor(0.3074, grad_fn=<NegBackward0>)\n",
            "Time 31.447539567947388 loss tensor(0.3070, grad_fn=<NegBackward0>)\n",
            "Time 31.496968507766724 loss tensor(0.3066, grad_fn=<NegBackward0>)\n",
            "Time 31.544811964035034 loss tensor(0.3062, grad_fn=<NegBackward0>)\n",
            "Time 31.590407371520996 loss tensor(0.3058, grad_fn=<NegBackward0>)\n",
            "Time 31.640722274780273 loss tensor(0.3053, grad_fn=<NegBackward0>)\n",
            "Time 31.687552452087402 loss tensor(0.3049, grad_fn=<NegBackward0>)\n",
            "Time 31.744216203689575 loss tensor(0.3045, grad_fn=<NegBackward0>)\n",
            "Time 31.788206100463867 loss tensor(0.3041, grad_fn=<NegBackward0>)\n",
            "Time 31.831995487213135 loss tensor(0.3037, grad_fn=<NegBackward0>)\n",
            "Time 31.88195514678955 loss tensor(0.3033, grad_fn=<NegBackward0>)\n",
            "Time 31.9278826713562 loss tensor(0.3029, grad_fn=<NegBackward0>)\n",
            "Time 31.97197723388672 loss tensor(0.3025, grad_fn=<NegBackward0>)\n",
            "Time 32.02305746078491 loss tensor(0.3021, grad_fn=<NegBackward0>)\n",
            "Time 32.08581733703613 loss tensor(0.3016, grad_fn=<NegBackward0>)\n",
            "Time 32.14015078544617 loss tensor(0.3012, grad_fn=<NegBackward0>)\n",
            "Time 32.186444997787476 loss tensor(0.3008, grad_fn=<NegBackward0>)\n",
            "Time 32.23267650604248 loss tensor(0.3004, grad_fn=<NegBackward0>)\n",
            "Time 32.28002953529358 loss tensor(0.3000, grad_fn=<NegBackward0>)\n",
            "Time 32.33525490760803 loss tensor(0.2996, grad_fn=<NegBackward0>)\n",
            "Final execution time 32.33525490760803\n",
            "Number of iterations 399\n",
            "Mean execution time of an iteration 0.08104073911681212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "XNQaGo06Ta49",
        "outputId": "730f42ae-957a-479b-b55a-bac1d5609bd9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fed353d8f70>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "6G3P5f8LTdAY",
        "outputId": "244a60e1-ebe2-4198-ed97-4aa3d29bb92c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fed353760a0>]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(errors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "l0LtcyhPTeh8",
        "outputId": "b3400326-351f-454d-88b6-d4e43616c82d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fed34e8c3d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}