{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+RG74YHjcd81VRtaY+qCL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidraamirez/GradientWithoutBackpropagation/blob/main/LogisticRegression_fwd_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Oyi7eax2JnEO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tqdm\n",
        "import torch.distributions as distr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torchmetrics --quiet"
      ],
      "metadata": {
        "id": "GVuIq3iFKJ9l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "import torchvision\n",
        "from torchvision import transforms as T"
      ],
      "metadata": {
        "id": "E7LCz1JiKMSj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and preprocessing the data"
      ],
      "metadata": {
        "id": "uclnxVteKfnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "train_data = torchvision.datasets.KMNIST('./data', train=True, download=True)"
      ],
      "metadata": {
        "id": "fZMg1kYeKS4U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This loads data with both data conversion.\n",
        "train_data = torchvision.datasets.KMNIST('./data', train=True, transform=T.ToTensor())"
      ],
      "metadata": {
        "id": "jaA81GgqLK8s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaders are used to shuffle, batch, and possibly sample the elements of the dataset\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "gbmkPVXBLNUM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = next(iter(train_loader))\n",
        "print(xb.shape)\n",
        "print(yb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISJ8BgIMLPWX",
        "outputId": "5348b97c-c36c-4565-ce4d-5c9c33cfd76c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1, 28, 28])\n",
            "torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the test data is similar, but (a) we do not apply data augmentation,\n",
        "# and (b) we do not shuffle when building the mini-batches.\n",
        "test_data = torchvision.datasets.KMNIST('./data', train=False, transform=T.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "m7oWyysXLhH9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Logistic Regression"
      ],
      "metadata": {
        "id": "KJO3tb6HLkD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "SbAXbIFTLjW-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLogisticRegression(nn.Module):\n",
        "  def __init__(self, input_size, w, b):\n",
        "    super(SimpleLogisticRegression, self).__init__()\n",
        "    input_size = 28\n",
        "    self.linear = nn.Linear(input_size, 28*28)\n",
        "    self.linear.weight = nn.Parameter(w)\n",
        "    self.linear.bias = nn.Parameter(b)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.reshape(-1, 28*28)\n",
        "    return torch.softmax(self.linear(x), 1)"
      ],
      "metadata": {
        "id": "F_qTD2ovLn2Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We check if CUDA is available. If you do not see it,\n",
        "# activate a GPU from Runtime >> Change runtime type and \n",
        "# restart the notebook.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9O-8lwQM3IK",
        "outputId": "9a3ab558-81fc-4824-fe64-0e6335027104"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the parameters"
      ],
      "metadata": {
        "id": "xANyM4TzM7YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We initialize the parameters randomly and the model with an input size\n",
        "w = torch.randn((10, 28*28), requires_grad=False)\n",
        "b = torch.randn(10, requires_grad=False)\n",
        "LG = SimpleLogisticRegression(1, w, b).to(device)"
      ],
      "metadata": {
        "id": "Au-0g48hM47D"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: we also need to move data when asking for a prediction\n",
        "LG(xb.to(device)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc8fw23PNqUQ",
        "outputId": "af4b4ea8-00ac-449f-fba7-33dece935f0b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate the network with forward gradient"
      ],
      "metadata": {
        "id": "VnUzjP_aWCdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(net, loader, device):\n",
        "  # A function that aggregates the accuracy over all mini-batches in the loader.\n",
        "  # See here for a quick-start on torchmetrics: https://torchmetrics.readthedocs.io/en/stable/pages/quickstart.html.\n",
        "  #acc = torchmetrics.Accuracy().to(device)\n",
        "  acc = torchmetrics.Accuracy('multiclass', num_classes=10).to(device)\n",
        "  for xb, yb in loader:\n",
        "      xb, yb = xb.to(device), yb.to(device)\n",
        "      ypred = LG(xb)\n",
        "      _ = acc(ypred, yb)\n",
        "  return acc.compute()"
      ],
      "metadata": {
        "id": "WtYvEF9VNqqZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average accuracy at initialization is 10% (random guessing).\n",
        "accuracy(LG, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bw1HBA6WGES",
        "outputId": "99e63675-f6f4-4de9-8a6a-7818d399eb6a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0812, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Cross Entropy"
      ],
      "metadata": {
        "id": "nZ0xyQDVWli6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: it is important to move the CNN to the device before initializing the optimizer,\n",
        "# since the optimizer also has a state that must be moved to the GPU.\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "et6ncKfAWPqW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beale_function(x):\n",
        "  return (torch.pow(torch.tensor([1.5])-x[0]+x[0]*x[1],2) + torch.pow(torch.tensor([2.25])-x[0]+x[0]*torch.pow(x[1],2),2)+torch.pow(torch.tensor([2.625])-x[0]+x[0]*torch.pow(x[1],3),2))"
      ],
      "metadata": {
        "id": "fW7DIT_bWrAv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rosenbrock_function(x):\n",
        "  sum=0\n",
        "  for i in range (x.size(0) -1):\n",
        "    sum += (100*torch.pow(x[i+1] - torch.pow(x[i], 2), 2) + torch.pow(x[i]-1, 2))\n",
        "  return sum"
      ],
      "metadata": {
        "id": "UWDzjXptWuAH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fwd_gradient(x, y):\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  l_rate0 = 0.0025\n",
        "  f = rosenbrock_function\n",
        "\n",
        "  #Parameters\n",
        "  w = torch.rand((8, 28*28), requires_grad=False)\n",
        "  w = torch.div(w, torch.norm(w, 2))\n",
        "\n",
        "  b = torch.rand(8, requires_grad=False)\n",
        "  b = torch.div(b, torch.norm(b, 2))\n",
        "\n",
        "  LG = SimpleLogisticRegression(1, w, b).to(device)\n",
        "  w1 = w.reshape(-1)\n",
        "  #w1 = torch.cat((w1, b), 0)\n",
        "\n",
        "  error = torch.norm(LG(x) - y, 2)\n",
        "  t=torch.tensor([0])\n",
        "\n",
        "  while (error>1e-3) :\n",
        "\n",
        "    t=t+1\n",
        "    v=torch.diagonal(torch.normal(torch.zeros_like(w1),torch.eye(w1.shape[0])))\n",
        "\n",
        "    ftw1 = f(w1)\n",
        "    ftb = f(b)\n",
        "    print('ftw1', ftw1)\n",
        "    print('ftb', ftb)\n",
        "    dt=torch.tensor(torch.autograd.functional.jvp(f,w1,v)[1])\n",
        "    print('dt', dt)\n",
        "    gt=v*dt\n",
        "    w1 = w1 - l_rate0*gt\n",
        "\n",
        "    w = w1[:8*28*28]\n",
        "    w = w.reshape(-1, 28*28)\n",
        "    b = w1[(8*28*28):]\n",
        "\n",
        "    LG = SimpleLogisticRegression(1, w, b).to(device)\n",
        "    error = torch.norm(LG(x) - y, 2)\n",
        "    print('error', error)\n",
        "\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "1wUCVnwXWzQF"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1):\n",
        "\n",
        "  LG.train()\n",
        "  for i in range(1):\n",
        "    xb, yb = next(iter(train_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    w, b = train_fwd_gradient(xb, yb)\n",
        "    LG = SimpleLogisticRegression(1, w, b)"
      ],
      "metadata": {
        "id": "aDYtH6ud1Dwz",
        "outputId": "d7005f30-7249-4004-a33e-a112c0c5ed8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ft tensor(6314.2305)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-3c6ee2632a55>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dt=torch.tensor(torch.autograd.functional.jvp(f,w1,v)[1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt tensor(12.9284)\n",
            "error tensor(44.9298, device='cuda:0', grad_fn=<NormBackward1>)\n",
            "ft tensor(6951.1831)\n",
            "dt tensor(-596.4167)\n",
            "error tensor(44.7646, device='cuda:0', grad_fn=<NormBackward1>)\n",
            "ft tensor(10729264.)\n",
            "dt tensor(914927.)\n",
            "error tensor(44.6990, device='cuda:0', grad_fn=<NormBackward1>)\n",
            "ft tensor(5.0900e+19)\n",
            "dt tensor(-1.2355e+15)\n",
            "error tensor(45.0111, device='cuda:0', grad_fn=<NormBackward1>)\n",
            "ft tensor(inf)\n",
            "dt tensor(nan)\n",
            "error tensor(nan, device='cuda:0', grad_fn=<NormBackward1>)\n"
          ]
        }
      ]
    }
  ]
}